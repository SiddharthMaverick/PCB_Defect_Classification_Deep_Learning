{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohxANbt49M77",
    "outputId": "ce4b9b21-8c5e-429c-b478-206df47c8098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pcb_defects_classification'...\n",
      "remote: Enumerating objects: 14628, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 14628 (delta 3), reused 7 (delta 3), pack-reused 14616\u001b[K\n",
      "Receiving objects: 100% (14628/14628), 95.24 MiB | 28.10 MiB/s, done.\n",
      "Resolving deltas: 100% (4117/4117), done.\n",
      "Updating files: 100% (14539/14539), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SiddharthMaverick/PCB_Defect_Classification_Deep_Learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iQknNn19M0y",
    "outputId": "d14a9015-1c87-4026-93ae-ea8b237fe88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/pcb_defects_classification\n"
     ]
    }
   ],
   "source": [
    "%cd pcb_defects_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AcwTRunv9HEw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tools import config\n",
    "\n",
    "if not os.path.exists(config.DEFECTS_PATH):\n",
    "    !python \"tools/extracted_defetcs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lIbxwBCw9HEx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "from tensorflow.keras.applications import EfficientNetB0 # Change import\n",
    "from tensorflow.keras.layers import Flatten, Dropout, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hC5jOCHz9HEy"
   },
   "outputs": [],
   "source": [
    "datagenAug = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "\tzoom_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tvertical_flip=True,\n",
    "\tvalidation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QIno6WM9HEy",
    "outputId": "73fa3d8d-74e2-4e12-a70f-3033b55e68d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 6 classes.\n",
      "Found 2001 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "trainGen = datagenAug.flow_from_directory(\n",
    "    config.DEFECTS_PATH, classes=config.CLASSES,\n",
    "    target_size=(224, 224), class_mode=\"categorical\",\n",
    "    batch_size=32, subset=\"training\")\n",
    "\n",
    "testGen = datagenAug.flow_from_directory(\n",
    "    config.DEFECTS_PATH, classes=config.CLASSES,\n",
    "    target_size=(224, 224), class_mode=\"categorical\",\n",
    "    batch_size=32, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6r_GNpi9HEy",
    "outputId": "7f9eff00-e1b3-4b7b-c97d-1d1fbf6ee02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load EfficientNetB0 model\n",
    "efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "for layer in efficientnet_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "head = efficientnet_model.output\n",
    "flatten = Flatten()(head)\n",
    "fc = Dense(512, activation='relu')(flatten)\n",
    "output = Dense(len(trainGen.class_indices), activation=\"softmax\")(fc)\n",
    "\n",
    "model = Model(inputs=efficientnet_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hMFTIu389HEy"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wsOZVP9X9HEy"
   },
   "outputs": [],
   "source": [
    "#checkpoint = ModelCheckpoint(os.path.sep.join([config.OUTPUT_PATH, \"vgg.h5\"]), monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "checkpoint = ModelCheckpoint(os.path.sep.join([config.OUTPUT_PATH, \"efficient.keras\"]), monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "\n",
    "early = EarlyStopping(monitor='accuracy', min_delta=0, patience=100, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSSkWxga9HEy",
    "outputId": "2e6b8b80-a73c-4541-bbf7-e2f994559e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " 10/250 [>.............................] - ETA: 1:13 - loss: 0.0840 - accuracy: 0.9719\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:15 - loss: 0.0772 - accuracy: 0.9744\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:16 - loss: 0.0991 - accuracy: 0.9714\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:17 - loss: 0.1062 - accuracy: 0.9712\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:17 - loss: 0.1136 - accuracy: 0.9710\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:18 - loss: 0.1103 - accuracy: 0.9708\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:18 - loss: 0.1166 - accuracy: 0.9707\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:19 - loss: 0.1176 - accuracy: 0.9669\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:22 - loss: 0.1121 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:25 - loss: 0.1071 - accuracy: 0.9704\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:27 - loss: 0.1084 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:28 - loss: 0.1077 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:28 - loss: 0.1068 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:27 - loss: 0.1034 - accuracy: 0.9701\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:27 - loss: 0.1066 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:26 - loss: 0.1056 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:26 - loss: 0.1021 - accuracy: 0.9700\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:25 - loss: 0.0987 - accuracy: 0.9711\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:25 - loss: 0.1001 - accuracy: 0.9699\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:24 - loss: 0.1022 - accuracy: 0.9688\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:24 - loss: 0.1114 - accuracy: 0.9656\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:23 - loss: 0.1094 - accuracy: 0.9657\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:23 - loss: 0.1089 - accuracy: 0.9658\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:22 - loss: 0.1064 - accuracy: 0.9669\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:22 - loss: 0.1038 - accuracy: 0.9678\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:22 - loss: 0.1042 - accuracy: 0.9661\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:21 - loss: 0.1117 - accuracy: 0.9653\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:21 - loss: 0.1090 - accuracy: 0.9662\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:20 - loss: 0.1093 - accuracy: 0.9655\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:20 - loss: 0.1075 - accuracy: 0.9655\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:20 - loss: 0.1056 - accuracy: 0.9664\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:19 - loss: 0.1041 - accuracy: 0.9672\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:19 - loss: 0.1042 - accuracy: 0.9665\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:18 - loss: 0.1029 - accuracy: 0.9666\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:18 - loss: 0.1051 - accuracy: 0.9659\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:17 - loss: 0.1106 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:17 - loss: 0.1116 - accuracy: 0.9626\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:17 - loss: 0.1115 - accuracy: 0.9628\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:17 - loss: 0.1110 - accuracy: 0.9629\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:18 - loss: 0.1094 - accuracy: 0.9636\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:18 - loss: 0.1100 - accuracy: 0.9631\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:19 - loss: 0.1104 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:18 - loss: 0.1119 - accuracy: 0.9627\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:18 - loss: 0.1108 - accuracy: 0.9629\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:17 - loss: 0.1094 - accuracy: 0.9630\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:17 - loss: 0.1092 - accuracy: 0.9625\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:16 - loss: 0.1094 - accuracy: 0.9615\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:16 - loss: 0.1077 - accuracy: 0.9622\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:15 - loss: 0.1074 - accuracy: 0.9623\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:15 - loss: 0.1088 - accuracy: 0.9624\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:14 - loss: 0.1076 - accuracy: 0.9630\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:14 - loss: 0.1065 - accuracy: 0.9631\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:14 - loss: 0.1058 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:12 - loss: 0.1052 - accuracy: 0.9634\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:12 - loss: 0.1056 - accuracy: 0.9635\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:12 - loss: 0.1042 - accuracy: 0.9641\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:11 - loss: 0.1042 - accuracy: 0.9641\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:11 - loss: 0.1044 - accuracy: 0.9642\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:10 - loss: 0.1091 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:10 - loss: 0.1100 - accuracy: 0.9630\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:09 - loss: 0.1089 - accuracy: 0.9635\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:09 - loss: 0.1086 - accuracy: 0.9631\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:09 - loss: 0.1088 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:08 - loss: 0.1078 - accuracy: 0.9637\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:08 - loss: 0.1078 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:07 - loss: 0.1064 - accuracy: 0.9643\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:07 - loss: 0.1055 - accuracy: 0.9648\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:07 - loss: 0.1051 - accuracy: 0.9648\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:06 - loss: 0.1043 - accuracy: 0.9653\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:06 - loss: 0.1119 - accuracy: 0.9649\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:06 - loss: 0.1118 - accuracy: 0.9646\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:06 - loss: 0.1105 - accuracy: 0.9650\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:06 - loss: 0.1093 - accuracy: 0.9654\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:06 - loss: 0.1083 - accuracy: 0.9659\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:06 - loss: 0.1080 - accuracy: 0.9655\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:05 - loss: 0.1069 - accuracy: 0.9659\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:05 - loss: 0.1072 - accuracy: 0.9656\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:04 - loss: 0.1060 - accuracy: 0.9660\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:04 - loss: 0.1049 - accuracy: 0.9664\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:03 - loss: 0.1039 - accuracy: 0.9668\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:03 - loss: 0.1040 - accuracy: 0.9664\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:02 - loss: 0.1031 - accuracy: 0.9668\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:02 - loss: 0.1031 - accuracy: 0.9668\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:02 - loss: 0.1026 - accuracy: 0.9668\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:01 - loss: 0.1027 - accuracy: 0.9665\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:01 - loss: 0.1021 - accuracy: 0.9666\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:00 - loss: 0.1021 - accuracy: 0.9666\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:00 - loss: 0.1017 - accuracy: 0.9666\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 59s - loss: 0.1018 - accuracy: 0.9663 \n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 59s - loss: 0.1025 - accuracy: 0.9657\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 59s - loss: 0.1040 - accuracy: 0.9651\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 58s - loss: 0.1044 - accuracy: 0.9651\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 58s - loss: 0.1035 - accuracy: 0.9655\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 57s - loss: 0.1040 - accuracy: 0.9649\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 57s - loss: 0.1039 - accuracy: 0.9649\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 56s - loss: 0.1054 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 56s - loss: 0.1051 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 56s - loss: 0.1045 - accuracy: 0.9642\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 55s - loss: 0.1038 - accuracy: 0.9645\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 55s - loss: 0.1042 - accuracy: 0.9645\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 55s - loss: 0.1037 - accuracy: 0.9646\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 55s - loss: 0.1042 - accuracy: 0.9643\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 54s - loss: 0.1051 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 54s - loss: 0.1044 - accuracy: 0.9641\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 54s - loss: 0.1052 - accuracy: 0.9639\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 53s - loss: 0.1056 - accuracy: 0.9634\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 53s - loss: 0.1063 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 53s - loss: 0.1074 - accuracy: 0.9629\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 52s - loss: 0.1069 - accuracy: 0.9630\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 52s - loss: 0.1062 - accuracy: 0.9633\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 51s - loss: 0.1055 - accuracy: 0.9636\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 51s - loss: 0.1050 - accuracy: 0.9639\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 50s - loss: 0.1045 - accuracy: 0.9642\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 50s - loss: 0.1042 - accuracy: 0.9642\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 50s - loss: 0.1046 - accuracy: 0.9640\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 49s - loss: 0.1043 - accuracy: 0.9641\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 49s - loss: 0.1042 - accuracy: 0.9641\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 48s - loss: 0.1039 - accuracy: 0.9639\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 48s - loss: 0.1050 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 47s - loss: 0.1046 - accuracy: 0.9632\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 47s - loss: 0.1040 - accuracy: 0.9635\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 47s - loss: 0.1035 - accuracy: 0.9638\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 46s - loss: 0.1056 - accuracy: 0.9631\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 46s - loss: 0.1072 - accuracy: 0.9629\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 45s - loss: 0.1077 - accuracy: 0.9627\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 45s - loss: 0.1093 - accuracy: 0.9623\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 45s - loss: 0.1111 - accuracy: 0.9619\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 44s - loss: 0.1113 - accuracy: 0.9617\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 44s - loss: 0.1105 - accuracy: 0.9620\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 43s - loss: 0.1108 - accuracy: 0.9621\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 43s - loss: 0.1108 - accuracy: 0.9621\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 43s - loss: 0.1109 - accuracy: 0.9619\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 43s - loss: 0.1115 - accuracy: 0.9618\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 42s - loss: 0.1129 - accuracy: 0.9612\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 42s - loss: 0.1132 - accuracy: 0.9608\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 42s - loss: 0.1128 - accuracy: 0.9610\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 41s - loss: 0.1129 - accuracy: 0.9609\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 41s - loss: 0.1134 - accuracy: 0.9609\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 40s - loss: 0.1132 - accuracy: 0.9610\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 40s - loss: 0.1136 - accuracy: 0.9606\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 40s - loss: 0.1142 - accuracy: 0.9603\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 39s - loss: 0.1150 - accuracy: 0.9601\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 39s - loss: 0.1156 - accuracy: 0.9602\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 38s - loss: 0.1182 - accuracy: 0.9598\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 38s - loss: 0.1204 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 38s - loss: 0.1205 - accuracy: 0.9591\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 37s - loss: 0.1218 - accuracy: 0.9588\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 37s - loss: 0.1210 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 36s - loss: 0.1207 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 36s - loss: 0.1231 - accuracy: 0.9586\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 35s - loss: 0.1231 - accuracy: 0.9586\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 35s - loss: 0.1245 - accuracy: 0.9583\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 35s - loss: 0.1243 - accuracy: 0.9584\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 34s - loss: 0.1239 - accuracy: 0.9584\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 34s - loss: 0.1242 - accuracy: 0.9583\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 33s - loss: 0.1241 - accuracy: 0.9582\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 33s - loss: 0.1252 - accuracy: 0.9579\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 33s - loss: 0.1247 - accuracy: 0.9579\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 32s - loss: 0.1243 - accuracy: 0.9580\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 32s - loss: 0.1242 - accuracy: 0.9581\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 31s - loss: 0.1240 - accuracy: 0.9581\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 31s - loss: 0.1244 - accuracy: 0.9580\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 31s - loss: 0.1250 - accuracy: 0.9579\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 30s - loss: 0.1252 - accuracy: 0.9578\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 30s - loss: 0.1267 - accuracy: 0.9575\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 30s - loss: 0.1260 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 29s - loss: 0.1255 - accuracy: 0.9578\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 29s - loss: 0.1254 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 28s - loss: 0.1250 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 28s - loss: 0.1256 - accuracy: 0.9574\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 28s - loss: 0.1250 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 27s - loss: 0.1248 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 27s - loss: 0.1244 - accuracy: 0.9578\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 26s - loss: 0.1246 - accuracy: 0.9577\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 26s - loss: 0.1239 - accuracy: 0.9579\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 26s - loss: 0.1242 - accuracy: 0.9576\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 25s - loss: 0.1238 - accuracy: 0.9579\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 25s - loss: 0.1232 - accuracy: 0.9581\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 24s - loss: 0.1227 - accuracy: 0.9583\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 24s - loss: 0.1222 - accuracy: 0.9585\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 23s - loss: 0.1217 - accuracy: 0.9587\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 23s - loss: 0.1229 - accuracy: 0.9586\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 23s - loss: 0.1232 - accuracy: 0.9585\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 22s - loss: 0.1228 - accuracy: 0.9586\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 22s - loss: 0.1228 - accuracy: 0.9586\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 21s - loss: 0.1227 - accuracy: 0.9587\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 21s - loss: 0.1222 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 21s - loss: 0.1233 - accuracy: 0.9588\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 20s - loss: 0.1232 - accuracy: 0.9587\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 20s - loss: 0.1231 - accuracy: 0.9587\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 19s - loss: 0.1226 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 19s - loss: 0.1224 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.1219 - accuracy: 0.9592\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 18s - loss: 0.1215 - accuracy: 0.9592\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 18s - loss: 0.1217 - accuracy: 0.9591\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.1216 - accuracy: 0.9592\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 17s - loss: 0.1211 - accuracy: 0.9594\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.1216 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 16s - loss: 0.1212 - accuracy: 0.9592\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 16s - loss: 0.1211 - accuracy: 0.9591\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.1211 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 15s - loss: 0.1215 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.1216 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 14s - loss: 0.1215 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 14s - loss: 0.1217 - accuracy: 0.9588\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.1214 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 13s - loss: 0.1210 - accuracy: 0.9591\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.1217 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 12s - loss: 0.1214 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.1213 - accuracy: 0.9589\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.1210 - accuracy: 0.9590\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 11s - loss: 0.1206 - accuracy: 0.9592\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.1203 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 10s - loss: 0.1205 - accuracy: 0.9594\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.1202 - accuracy: 0.9594\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.1199 - accuracy: 0.9596\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 9s - loss: 0.1197 - accuracy: 0.9597 \n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.1203 - accuracy: 0.9594\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 8s - loss: 0.1206 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.1204 - accuracy: 0.9594\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 7s - loss: 0.1203 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.1201 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.1198 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 6s - loss: 0.1202 - accuracy: 0.9591\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.1198 - accuracy: 0.9593\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.1193 - accuracy: 0.9595\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.1192 - accuracy: 0.9595\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.1187 - accuracy: 0.9597\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.1189 - accuracy: 0.9596\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.1184 - accuracy: 0.9598\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.1180 - accuracy: 0.9599\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.1192 - accuracy: 0.9597\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.1193 - accuracy: 0.9597\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.1194 - accuracy: 0.9598\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.1190 - accuracy: 0.9599\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.1194 - accuracy: 0.9598\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.1204 - accuracy: 0.9598\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.1200 - accuracy: 0.9599\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9600\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9596\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 138s 497ms/step - loss: 0.1220 - accuracy: 0.9596 - val_loss: 0.0727 - val_accuracy: 0.9808\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:50 - loss: 0.4684 - accuracy: 0.9062\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:34 - loss: 0.2371 - accuracy: 0.9531\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:32 - loss: 0.2520 - accuracy: 0.9375\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:32 - loss: 0.2267 - accuracy: 0.9375\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:30 - loss: 0.1852 - accuracy: 0.9500\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:29 - loss: 0.2651 - accuracy: 0.9479\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:29 - loss: 0.2424 - accuracy: 0.9509\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:29 - loss: 0.2174 - accuracy: 0.9531\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:28 - loss: 0.2141 - accuracy: 0.9514\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:28 - loss: 0.2074 - accuracy: 0.9500\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:27 - loss: 0.2171 - accuracy: 0.9403\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:28 - loss: 0.2044 - accuracy: 0.9427\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:31 - loss: 0.2000 - accuracy: 0.9447\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:35 - loss: 0.1869 - accuracy: 0.9487\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:38 - loss: 0.1969 - accuracy: 0.9479\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:40 - loss: 0.1860 - accuracy: 0.9512\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:39 - loss: 0.1778 - accuracy: 0.9540\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:38 - loss: 0.1793 - accuracy: 0.9531\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:37 - loss: 0.1715 - accuracy: 0.9556\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:36 - loss: 0.1656 - accuracy: 0.9578\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:35 - loss: 0.1722 - accuracy: 0.9539\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:34 - loss: 0.1657 - accuracy: 0.9560\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:33 - loss: 0.1612 - accuracy: 0.9579\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:33 - loss: 0.1604 - accuracy: 0.9583\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:32 - loss: 0.1555 - accuracy: 0.9600\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:31 - loss: 0.1508 - accuracy: 0.9603\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:31 - loss: 0.1526 - accuracy: 0.9595\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:30 - loss: 0.1577 - accuracy: 0.9576\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:29 - loss: 0.1545 - accuracy: 0.9580\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:29 - loss: 0.1498 - accuracy: 0.9594\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:28 - loss: 0.1469 - accuracy: 0.9597\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:27 - loss: 0.1454 - accuracy: 0.9600\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:27 - loss: 0.1446 - accuracy: 0.9593\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:27 - loss: 0.1413 - accuracy: 0.9605\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:26 - loss: 0.1403 - accuracy: 0.9607\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:26 - loss: 0.1367 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:25 - loss: 0.1348 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:25 - loss: 0.1352 - accuracy: 0.9613\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:24 - loss: 0.1321 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:24 - loss: 0.1328 - accuracy: 0.9609\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:23 - loss: 0.1336 - accuracy: 0.9604\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:23 - loss: 0.1338 - accuracy: 0.9591\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:23 - loss: 0.1318 - accuracy: 0.9593\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:24 - loss: 0.1354 - accuracy: 0.9574\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:24 - loss: 0.1326 - accuracy: 0.9583\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:25 - loss: 0.1302 - accuracy: 0.9592\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:24 - loss: 0.1276 - accuracy: 0.9601\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:24 - loss: 0.1261 - accuracy: 0.9603\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:23 - loss: 0.1247 - accuracy: 0.9605\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:22 - loss: 0.1224 - accuracy: 0.9613\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:22 - loss: 0.1224 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:21 - loss: 0.1205 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:21 - loss: 0.1200 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:20 - loss: 0.1204 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:20 - loss: 0.1184 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:19 - loss: 0.1167 - accuracy: 0.9632\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:19 - loss: 0.1158 - accuracy: 0.9633\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:18 - loss: 0.1138 - accuracy: 0.9639\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:18 - loss: 0.1147 - accuracy: 0.9635\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:17 - loss: 0.1136 - accuracy: 0.9641\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:17 - loss: 0.1145 - accuracy: 0.9636\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:16 - loss: 0.1136 - accuracy: 0.9637\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:16 - loss: 0.1120 - accuracy: 0.9643\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:15 - loss: 0.1137 - accuracy: 0.9639\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:15 - loss: 0.1124 - accuracy: 0.9644\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:14 - loss: 0.1109 - accuracy: 0.9650\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:14 - loss: 0.1114 - accuracy: 0.9646\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:13 - loss: 0.1105 - accuracy: 0.9651\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:12 - loss: 0.1100 - accuracy: 0.9653\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:12 - loss: 0.1090 - accuracy: 0.9658\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:11 - loss: 0.1092 - accuracy: 0.9658\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:11 - loss: 0.1086 - accuracy: 0.9658\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:11 - loss: 0.1113 - accuracy: 0.9655\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:11 - loss: 0.1103 - accuracy: 0.9659\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:11 - loss: 0.1108 - accuracy: 0.9655\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.1104 - accuracy: 0.9652\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:11 - loss: 0.1123 - accuracy: 0.9644\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.1114 - accuracy: 0.9649\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:10 - loss: 0.1129 - accuracy: 0.9641\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.1135 - accuracy: 0.9638\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:09 - loss: 0.1121 - accuracy: 0.9642\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.1138 - accuracy: 0.9631\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.1144 - accuracy: 0.9632\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.1133 - accuracy: 0.9636\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.1195 - accuracy: 0.9630\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:07 - loss: 0.1228 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:06 - loss: 0.1271 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:06 - loss: 0.1265 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:05 - loss: 0.1264 - accuracy: 0.9611\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:05 - loss: 0.1259 - accuracy: 0.9612\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:04 - loss: 0.1246 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:04 - loss: 0.1235 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:03 - loss: 0.1242 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:03 - loss: 0.1232 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:02 - loss: 0.1226 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:02 - loss: 0.1225 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:01 - loss: 0.1216 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:01 - loss: 0.1207 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:01 - loss: 0.1203 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:00 - loss: 0.1212 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:00 - loss: 0.1224 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 59s - loss: 0.1222 - accuracy: 0.9612 \n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 59s - loss: 0.1224 - accuracy: 0.9609\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 59s - loss: 0.1217 - accuracy: 0.9613\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 59s - loss: 0.1222 - accuracy: 0.9611\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 58s - loss: 0.1220 - accuracy: 0.9609\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 58s - loss: 0.1216 - accuracy: 0.9606\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.1212 - accuracy: 0.9607\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 57s - loss: 0.1216 - accuracy: 0.9602\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.1219 - accuracy: 0.9603\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 56s - loss: 0.1219 - accuracy: 0.9601\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 56s - loss: 0.1212 - accuracy: 0.9604\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.1206 - accuracy: 0.9608\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 55s - loss: 0.1202 - accuracy: 0.9609\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 55s - loss: 0.1192 - accuracy: 0.9612\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 54s - loss: 0.1185 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 54s - loss: 0.1178 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 53s - loss: 0.1184 - accuracy: 0.9611\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 53s - loss: 0.1176 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 52s - loss: 0.1176 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 52s - loss: 0.1168 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 51s - loss: 0.1167 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 51s - loss: 0.1167 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 51s - loss: 0.1190 - accuracy: 0.9612\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 50s - loss: 0.1185 - accuracy: 0.9613\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 50s - loss: 0.1188 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 49s - loss: 0.1188 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 49s - loss: 0.1189 - accuracy: 0.9612\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 49s - loss: 0.1183 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 48s - loss: 0.1180 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 48s - loss: 0.1174 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 47s - loss: 0.1174 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 47s - loss: 0.1167 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 47s - loss: 0.1176 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 46s - loss: 0.1168 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 46s - loss: 0.1163 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.1166 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 45s - loss: 0.1174 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 45s - loss: 0.1166 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.1177 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 44s - loss: 0.1172 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.1179 - accuracy: 0.9613\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 43s - loss: 0.1177 - accuracy: 0.9614\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.1169 - accuracy: 0.9616\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.1173 - accuracy: 0.9615\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 42s - loss: 0.1166 - accuracy: 0.9617\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 42s - loss: 0.1161 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 41s - loss: 0.1154 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 41s - loss: 0.1148 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 40s - loss: 0.1146 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 40s - loss: 0.1144 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.1138 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 39s - loss: 0.1159 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 39s - loss: 0.1160 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 38s - loss: 0.1159 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 38s - loss: 0.1152 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 37s - loss: 0.1157 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 37s - loss: 0.1157 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.1158 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 36s - loss: 0.1158 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 36s - loss: 0.1156 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 35s - loss: 0.1154 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 35s - loss: 0.1148 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.1143 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 34s - loss: 0.1155 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 34s - loss: 0.1159 - accuracy: 0.9618\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.1156 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 33s - loss: 0.1154 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.1149 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 32s - loss: 0.1164 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.1165 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 31s - loss: 0.1161 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 31s - loss: 0.1165 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.1175 - accuracy: 0.9620\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 30s - loss: 0.1170 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.1166 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 29s - loss: 0.1160 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 29s - loss: 0.1156 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.1151 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 28s - loss: 0.1147 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.1143 - accuracy: 0.9631\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 27s - loss: 0.1151 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 27s - loss: 0.1153 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 26s - loss: 0.1159 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 26s - loss: 0.1159 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.1155 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 25s - loss: 0.1153 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.1149 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 24s - loss: 0.1156 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 24s - loss: 0.1157 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.1155 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 23s - loss: 0.1151 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.1151 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 22s - loss: 0.1152 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.1155 - accuracy: 0.9619\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.1149 - accuracy: 0.9621\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 21s - loss: 0.1143 - accuracy: 0.9623\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.1150 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 20s - loss: 0.1145 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.1147 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.1154 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.1149 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.1145 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 18s - loss: 0.1146 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.1144 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.1146 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.1143 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.1140 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 16s - loss: 0.1136 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.1134 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 15s - loss: 0.1132 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.1129 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.1126 - accuracy: 0.9631\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 14s - loss: 0.1127 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.1130 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 13s - loss: 0.1130 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.1129 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.1125 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.1122 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.1121 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 11s - loss: 0.1120 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.1117 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.1114 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.1111 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.1109 - accuracy: 0.9630\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 9s - loss: 0.1113 - accuracy: 0.9627 \n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.1110 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.1110 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.1108 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.1112 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.1108 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.1104 - accuracy: 0.9630\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 6s - loss: 0.1115 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.1124 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.1125 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.1122 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.1131 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.1128 - accuracy: 0.9622\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.1125 - accuracy: 0.9624\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.1122 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.1122 - accuracy: 0.9626\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.1117 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.1116 - accuracy: 0.9627\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.1113 - accuracy: 0.9629\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.1114 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.1111 - accuracy: 0.9628\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.1108 - accuracy: 0.9630\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9625\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 126s 505ms/step - loss: 0.1113 - accuracy: 0.9627 - val_loss: 0.0531 - val_accuracy: 0.9798\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:45 - loss: 0.0709 - accuracy: 0.9688\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:34 - loss: 0.1033 - accuracy: 0.9688\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:33 - loss: 0.0831 - accuracy: 0.9688\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:34 - loss: 0.0916 - accuracy: 0.9531\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:32 - loss: 0.0908 - accuracy: 0.9563\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:32 - loss: 0.0912 - accuracy: 0.9531\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:32 - loss: 0.1043 - accuracy: 0.9464\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:31 - loss: 0.1110 - accuracy: 0.9492\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:31 - loss: 0.1036 - accuracy: 0.9549\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:31 - loss: 0.1002 - accuracy: 0.9531\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:31 - loss: 0.0938 - accuracy: 0.9574\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:30 - loss: 0.0915 - accuracy: 0.9583\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:31 - loss: 0.0888 - accuracy: 0.9591\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:30 - loss: 0.0886 - accuracy: 0.9598\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:30 - loss: 0.0837 - accuracy: 0.9625\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:30 - loss: 0.0855 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:29 - loss: 0.0867 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:29 - loss: 0.0834 - accuracy: 0.9653\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:29 - loss: 0.0799 - accuracy: 0.9671\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:28 - loss: 0.0884 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:28 - loss: 0.0883 - accuracy: 0.9658\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:28 - loss: 0.1057 - accuracy: 0.9645\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:30 - loss: 0.1050 - accuracy: 0.9647\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:31 - loss: 0.1090 - accuracy: 0.9648\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:33 - loss: 0.1058 - accuracy: 0.9663\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:33 - loss: 0.1044 - accuracy: 0.9663\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:32 - loss: 0.1044 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:32 - loss: 0.1009 - accuracy: 0.9676\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:31 - loss: 0.1007 - accuracy: 0.9677\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:30 - loss: 0.1013 - accuracy: 0.9667\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:30 - loss: 0.1007 - accuracy: 0.9657\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:29 - loss: 0.1009 - accuracy: 0.9658\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:29 - loss: 0.1068 - accuracy: 0.9659\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:28 - loss: 0.1079 - accuracy: 0.9651\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:27 - loss: 0.1079 - accuracy: 0.9652\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:27 - loss: 0.1072 - accuracy: 0.9653\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:27 - loss: 0.1083 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:26 - loss: 0.1084 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:25 - loss: 0.1060 - accuracy: 0.9663\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:25 - loss: 0.1069 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:24 - loss: 0.1076 - accuracy: 0.9657\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:24 - loss: 0.1134 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:23 - loss: 0.1113 - accuracy: 0.9644\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:23 - loss: 0.1139 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:22 - loss: 0.1128 - accuracy: 0.9639\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:22 - loss: 0.1112 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:21 - loss: 0.1100 - accuracy: 0.9641\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:21 - loss: 0.1088 - accuracy: 0.9642\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:20 - loss: 0.1070 - accuracy: 0.9649\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:19 - loss: 0.1072 - accuracy: 0.9646\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:18 - loss: 0.1082 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:18 - loss: 0.1086 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:18 - loss: 0.1081 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:18 - loss: 0.1086 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:19 - loss: 0.1068 - accuracy: 0.9644\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:19 - loss: 0.1054 - accuracy: 0.9650\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:19 - loss: 0.1040 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:18 - loss: 0.1027 - accuracy: 0.9662\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:17 - loss: 0.1032 - accuracy: 0.9657\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:17 - loss: 0.1018 - accuracy: 0.9663\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:16 - loss: 0.1005 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:16 - loss: 0.0998 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:16 - loss: 0.0989 - accuracy: 0.9674\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:15 - loss: 0.0993 - accuracy: 0.9670\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:15 - loss: 0.0994 - accuracy: 0.9670\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:14 - loss: 0.1096 - accuracy: 0.9665\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:14 - loss: 0.1085 - accuracy: 0.9666\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:13 - loss: 0.1080 - accuracy: 0.9666\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:13 - loss: 0.1121 - accuracy: 0.9662\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:12 - loss: 0.1114 - accuracy: 0.9667\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:12 - loss: 0.1102 - accuracy: 0.9671\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:11 - loss: 0.1104 - accuracy: 0.9672\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:11 - loss: 0.1128 - accuracy: 0.9672\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:11 - loss: 0.1118 - accuracy: 0.9676\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:10 - loss: 0.1117 - accuracy: 0.9676\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:10 - loss: 0.1123 - accuracy: 0.9672\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:09 - loss: 0.1138 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:09 - loss: 0.1152 - accuracy: 0.9665\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:08 - loss: 0.1139 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:08 - loss: 0.1150 - accuracy: 0.9661\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:07 - loss: 0.1161 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:07 - loss: 0.1152 - accuracy: 0.9658\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:07 - loss: 0.1151 - accuracy: 0.9659\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.1153 - accuracy: 0.9659\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.1158 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:07 - loss: 0.1147 - accuracy: 0.9660\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.1135 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:06 - loss: 0.1138 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:06 - loss: 0.1132 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:05 - loss: 0.1126 - accuracy: 0.9664\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:05 - loss: 0.1114 - accuracy: 0.9668\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:04 - loss: 0.1127 - accuracy: 0.9665\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:04 - loss: 0.1126 - accuracy: 0.9665\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:03 - loss: 0.1116 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:03 - loss: 0.1111 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:02 - loss: 0.1104 - accuracy: 0.9669\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:02 - loss: 0.1159 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:01 - loss: 0.1169 - accuracy: 0.9650\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:01 - loss: 0.1158 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:01 - loss: 0.1171 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:00 - loss: 0.1169 - accuracy: 0.9651\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:00 - loss: 0.1167 - accuracy: 0.9649\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 59s - loss: 0.1169 - accuracy: 0.9649 \n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 59s - loss: 0.1181 - accuracy: 0.9649\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 58s - loss: 0.1170 - accuracy: 0.9653\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 58s - loss: 0.1161 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 58s - loss: 0.1158 - accuracy: 0.9653\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 57s - loss: 0.1151 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 57s - loss: 0.1160 - accuracy: 0.9648\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.1153 - accuracy: 0.9651\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 58s - loss: 0.1143 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 58s - loss: 0.1150 - accuracy: 0.9652\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 59s - loss: 0.1143 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 58s - loss: 0.1152 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 58s - loss: 0.1150 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 57s - loss: 0.1146 - accuracy: 0.9656\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 57s - loss: 0.1137 - accuracy: 0.9659\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 56s - loss: 0.1160 - accuracy: 0.9651\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 56s - loss: 0.1156 - accuracy: 0.9652\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 55s - loss: 0.1153 - accuracy: 0.9654\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 55s - loss: 0.1155 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 54s - loss: 0.1154 - accuracy: 0.9655\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 54s - loss: 0.1157 - accuracy: 0.9653\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 53s - loss: 0.1161 - accuracy: 0.9650\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 53s - loss: 0.1162 - accuracy: 0.9651\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 52s - loss: 0.1178 - accuracy: 0.9646\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 52s - loss: 0.1179 - accuracy: 0.9644\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 52s - loss: 0.1182 - accuracy: 0.9642\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 51s - loss: 0.1184 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 51s - loss: 0.1181 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 50s - loss: 0.1172 - accuracy: 0.9643\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 50s - loss: 0.1171 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 49s - loss: 0.1164 - accuracy: 0.9641\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 49s - loss: 0.1173 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 48s - loss: 0.1167 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 48s - loss: 0.1173 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 47s - loss: 0.1171 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 47s - loss: 0.1172 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 47s - loss: 0.1169 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 47s - loss: 0.1174 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 46s - loss: 0.1167 - accuracy: 0.9639\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 46s - loss: 0.1175 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 45s - loss: 0.1177 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 45s - loss: 0.1185 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 44s - loss: 0.1179 - accuracy: 0.9639\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 44s - loss: 0.1172 - accuracy: 0.9641\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 43s - loss: 0.1172 - accuracy: 0.9639\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 43s - loss: 0.1174 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 43s - loss: 0.1174 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 42s - loss: 0.1180 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 42s - loss: 0.1176 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 41s - loss: 0.1172 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 41s - loss: 0.1171 - accuracy: 0.9633\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 40s - loss: 0.1165 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 40s - loss: 0.1172 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.1167 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 39s - loss: 0.1168 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.1177 - accuracy: 0.9633\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 38s - loss: 0.1175 - accuracy: 0.9631\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 38s - loss: 0.1171 - accuracy: 0.9631\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 37s - loss: 0.1164 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 37s - loss: 0.1187 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.1183 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 36s - loss: 0.1181 - accuracy: 0.9631\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.1178 - accuracy: 0.9631\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 35s - loss: 0.1187 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 35s - loss: 0.1182 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 34s - loss: 0.1176 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 34s - loss: 0.1171 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 34s - loss: 0.1165 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 33s - loss: 0.1166 - accuracy: 0.9637\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 33s - loss: 0.1178 - accuracy: 0.9633\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 32s - loss: 0.1183 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 32s - loss: 0.1183 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 31s - loss: 0.1180 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 31s - loss: 0.1180 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 31s - loss: 0.1197 - accuracy: 0.9626\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 30s - loss: 0.1201 - accuracy: 0.9623\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 30s - loss: 0.1199 - accuracy: 0.9622\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.1198 - accuracy: 0.9622\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 29s - loss: 0.1192 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.1191 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 28s - loss: 0.1194 - accuracy: 0.9621\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.1196 - accuracy: 0.9622\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.1199 - accuracy: 0.9622\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 27s - loss: 0.1193 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.1200 - accuracy: 0.9621\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 26s - loss: 0.1196 - accuracy: 0.9621\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.1190 - accuracy: 0.9623\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 25s - loss: 0.1184 - accuracy: 0.9625\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.1181 - accuracy: 0.9626\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.1183 - accuracy: 0.9626\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 24s - loss: 0.1180 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.1179 - accuracy: 0.9625\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 23s - loss: 0.1190 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.1198 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 22s - loss: 0.1195 - accuracy: 0.9624\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.1191 - accuracy: 0.9626\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.1196 - accuracy: 0.9627\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 21s - loss: 0.1193 - accuracy: 0.9627\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.1192 - accuracy: 0.9626\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 20s - loss: 0.1188 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.1182 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.1179 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 19s - loss: 0.1183 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.1190 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 18s - loss: 0.1198 - accuracy: 0.9627\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.1203 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.1201 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.1199 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.1203 - accuracy: 0.9627\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 16s - loss: 0.1200 - accuracy: 0.9627\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.1198 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.1195 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.1192 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.1190 - accuracy: 0.9629\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.1185 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.1182 - accuracy: 0.9631\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 13s - loss: 0.1185 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.1181 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.1178 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.1174 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.1172 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.1180 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.1175 - accuracy: 0.9630\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 10s - loss: 0.1181 - accuracy: 0.9628\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.1177 - accuracy: 0.9630 \n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.1173 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.1169 - accuracy: 0.9633\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.1168 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 8s - loss: 0.1165 - accuracy: 0.9632\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.1161 - accuracy: 0.9634\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.1156 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.1153 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.1153 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.1152 - accuracy: 0.9635\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.1147 - accuracy: 0.9636\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 5s - loss: 0.1143 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.1139 - accuracy: 0.9639\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.1145 - accuracy: 0.9638\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.1142 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.1140 - accuracy: 0.9640\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.1137 - accuracy: 0.9642\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.1134 - accuracy: 0.9643\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.1129 - accuracy: 0.9645\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.1125 - accuracy: 0.9646\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.1130 - accuracy: 0.9644\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9644\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9645\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 0.1128 - accuracy: 0.9647 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:45 - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:34 - loss: 0.1131 - accuracy: 0.9219\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:35 - loss: 0.1065 - accuracy: 0.9375\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:34 - loss: 0.0973 - accuracy: 0.9453\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:35 - loss: 0.1113 - accuracy: 0.9438\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:34 - loss: 0.1167 - accuracy: 0.9427\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:34 - loss: 0.1278 - accuracy: 0.9464\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:33 - loss: 0.1221 - accuracy: 0.9492\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:33 - loss: 0.1113 - accuracy: 0.9549\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:32 - loss: 0.1113 - accuracy: 0.9531\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:32 - loss: 0.1163 - accuracy: 0.9517\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:31 - loss: 0.1150 - accuracy: 0.9505\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:31 - loss: 0.1091 - accuracy: 0.9543\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:31 - loss: 0.1034 - accuracy: 0.9576\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:30 - loss: 0.1053 - accuracy: 0.9583\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:30 - loss: 0.1006 - accuracy: 0.9609\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:29 - loss: 0.0953 - accuracy: 0.9632\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:29 - loss: 0.0982 - accuracy: 0.9618\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:28 - loss: 0.0932 - accuracy: 0.9638\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:28 - loss: 0.0891 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:28 - loss: 0.0911 - accuracy: 0.9643\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:27 - loss: 0.0900 - accuracy: 0.9645\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:27 - loss: 0.0862 - accuracy: 0.9660\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:29 - loss: 0.0849 - accuracy: 0.9661\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:31 - loss: 0.0884 - accuracy: 0.9650\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:32 - loss: 0.0891 - accuracy: 0.9639\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:34 - loss: 0.0874 - accuracy: 0.9653\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:33 - loss: 0.0907 - accuracy: 0.9654\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:32 - loss: 0.0909 - accuracy: 0.9655\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:31 - loss: 0.0912 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.0945 - accuracy: 0.9627\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:30 - loss: 0.0975 - accuracy: 0.9629\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:29 - loss: 0.0962 - accuracy: 0.9631\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:29 - loss: 0.0958 - accuracy: 0.9623\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:28 - loss: 0.0935 - accuracy: 0.9634\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:27 - loss: 0.0953 - accuracy: 0.9618\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:27 - loss: 0.0937 - accuracy: 0.9628\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:26 - loss: 0.0928 - accuracy: 0.9630\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:26 - loss: 0.0911 - accuracy: 0.9639\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:25 - loss: 0.0896 - accuracy: 0.9648\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:25 - loss: 0.0902 - accuracy: 0.9642\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:24 - loss: 0.0909 - accuracy: 0.9635\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:24 - loss: 0.0897 - accuracy: 0.9637\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:23 - loss: 0.0878 - accuracy: 0.9645\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:23 - loss: 0.0873 - accuracy: 0.9646\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:22 - loss: 0.0866 - accuracy: 0.9647\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:22 - loss: 0.0874 - accuracy: 0.9641\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:21 - loss: 0.0863 - accuracy: 0.9648\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:21 - loss: 0.0862 - accuracy: 0.9649\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:20 - loss: 0.0850 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:20 - loss: 0.0851 - accuracy: 0.9657\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:19 - loss: 0.0850 - accuracy: 0.9657\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:19 - loss: 0.0836 - accuracy: 0.9664\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:19 - loss: 0.0821 - accuracy: 0.9670\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:20 - loss: 0.0855 - accuracy: 0.9659\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:20 - loss: 0.0844 - accuracy: 0.9665\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:20 - loss: 0.0846 - accuracy: 0.9666\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:19 - loss: 0.0847 - accuracy: 0.9666\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:19 - loss: 0.0865 - accuracy: 0.9661\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:18 - loss: 0.0895 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.0897 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:17 - loss: 0.0922 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0907 - accuracy: 0.9658\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:16 - loss: 0.0896 - accuracy: 0.9663\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0885 - accuracy: 0.9668\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:15 - loss: 0.0885 - accuracy: 0.9669\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0891 - accuracy: 0.9664\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:14 - loss: 0.0902 - accuracy: 0.9660\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0910 - accuracy: 0.9660\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:13 - loss: 0.0943 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0938 - accuracy: 0.9657\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0929 - accuracy: 0.9661\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0930 - accuracy: 0.9662\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0922 - accuracy: 0.9662\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:11 - loss: 0.0937 - accuracy: 0.9658\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.0932 - accuracy: 0.9659\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:10 - loss: 0.0941 - accuracy: 0.9651\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.0948 - accuracy: 0.9647\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:09 - loss: 0.0947 - accuracy: 0.9644\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.0936 - accuracy: 0.9648\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:08 - loss: 0.0927 - accuracy: 0.9653\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.0929 - accuracy: 0.9649\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.0922 - accuracy: 0.9654\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:08 - loss: 0.0920 - accuracy: 0.9654\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:08 - loss: 0.0928 - accuracy: 0.9651\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:08 - loss: 0.0936 - accuracy: 0.9648\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.0932 - accuracy: 0.9648\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:07 - loss: 0.0923 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:06 - loss: 0.0919 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0932 - accuracy: 0.9653\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:05 - loss: 0.0936 - accuracy: 0.9653\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0953 - accuracy: 0.9650\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0944 - accuracy: 0.9654\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:04 - loss: 0.0944 - accuracy: 0.9654\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0942 - accuracy: 0.9655\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:03 - loss: 0.0944 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0973 - accuracy: 0.9652\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:02 - loss: 0.0965 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0973 - accuracy: 0.9653\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:01 - loss: 0.0969 - accuracy: 0.9656\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0972 - accuracy: 0.9657\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:00 - loss: 0.0962 - accuracy: 0.9660\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:00 - loss: 0.0954 - accuracy: 0.9663\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0947 - accuracy: 0.9666\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 59s - loss: 0.0943 - accuracy: 0.9670 \n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 59s - loss: 0.0936 - accuracy: 0.9673\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 58s - loss: 0.0937 - accuracy: 0.9673\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.0929 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 57s - loss: 0.0928 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.0923 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 56s - loss: 0.0931 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 56s - loss: 0.0931 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.0926 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 56s - loss: 0.0920 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 55s - loss: 0.0927 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 55s - loss: 0.0949 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 55s - loss: 0.0949 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 54s - loss: 0.0951 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.0947 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 54s - loss: 0.0950 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.0944 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0937 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 52s - loss: 0.0955 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0961 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 51s - loss: 0.0954 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0954 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 50s - loss: 0.0964 - accuracy: 0.9670\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0966 - accuracy: 0.9668\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0963 - accuracy: 0.9668\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 49s - loss: 0.0958 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0955 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 48s - loss: 0.0966 - accuracy: 0.9669\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0977 - accuracy: 0.9666\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 47s - loss: 0.0970 - accuracy: 0.9669\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 47s - loss: 0.0964 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 46s - loss: 0.0964 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.0959 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0956 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 45s - loss: 0.0956 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0954 - accuracy: 0.9673\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 44s - loss: 0.0948 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.0947 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 43s - loss: 0.0943 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.0947 - accuracy: 0.9673\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0941 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 42s - loss: 0.0943 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 42s - loss: 0.0964 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0969 - accuracy: 0.9671\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 41s - loss: 0.0966 - accuracy: 0.9669\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0964 - accuracy: 0.9669\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 40s - loss: 0.0969 - accuracy: 0.9670\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0963 - accuracy: 0.9672\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0958 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 39s - loss: 0.0952 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0951 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 38s - loss: 0.0953 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0954 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 37s - loss: 0.0951 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.0946 - accuracy: 0.9680\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0952 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 36s - loss: 0.0970 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0970 - accuracy: 0.9673\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 35s - loss: 0.0965 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0959 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 34s - loss: 0.0954 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 34s - loss: 0.0960 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0960 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 33s - loss: 0.0958 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0968 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 32s - loss: 0.0973 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0974 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 31s - loss: 0.0972 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 31s - loss: 0.0970 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0974 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 30s - loss: 0.0970 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0970 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0972 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 29s - loss: 0.0967 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0964 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 28s - loss: 0.0968 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0971 - accuracy: 0.9674\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0966 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 27s - loss: 0.0972 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0967 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 26s - loss: 0.0962 - accuracy: 0.9680\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0967 - accuracy: 0.9676\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0964 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0962 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0964 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 24s - loss: 0.0959 - accuracy: 0.9678\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0967 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 23s - loss: 0.0974 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0971 - accuracy: 0.9675\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0967 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0970 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0976 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 21s - loss: 0.0975 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0971 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 20s - loss: 0.0972 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0973 - accuracy: 0.9677\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0968 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.0966 - accuracy: 0.9679\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0961 - accuracy: 0.9680\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 18s - loss: 0.0956 - accuracy: 0.9682\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0957 - accuracy: 0.9680\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0953 - accuracy: 0.9682\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.0950 - accuracy: 0.9682\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0947 - accuracy: 0.9684\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 16s - loss: 0.0943 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0942 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0941 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0950 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0946 - accuracy: 0.9687\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 14s - loss: 0.0943 - accuracy: 0.9688\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0940 - accuracy: 0.9690\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0936 - accuracy: 0.9691\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0934 - accuracy: 0.9692\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0934 - accuracy: 0.9692\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0932 - accuracy: 0.9692\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0928 - accuracy: 0.9694\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 11s - loss: 0.0929 - accuracy: 0.9694\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0927 - accuracy: 0.9694\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0925 - accuracy: 0.9695\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0927 - accuracy: 0.9695\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0924 - accuracy: 0.9696\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 9s - loss: 0.0930 - accuracy: 0.9694 \n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0927 - accuracy: 0.9694\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0932 - accuracy: 0.9694\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0937 - accuracy: 0.9692\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0948 - accuracy: 0.9691\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0948 - accuracy: 0.9691\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0948 - accuracy: 0.9689\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0945 - accuracy: 0.9691\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0950 - accuracy: 0.9689\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0952 - accuracy: 0.9688\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0955 - accuracy: 0.9687\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0953 - accuracy: 0.9688\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.0955 - accuracy: 0.9687\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0951 - accuracy: 0.9688\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0956 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0954 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0954 - accuracy: 0.9684\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0950 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0948 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0946 - accuracy: 0.9685\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0944 - accuracy: 0.9687\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0940 - accuracy: 0.9688\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9689\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9690\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 144s 578ms/step - loss: 0.0935 - accuracy: 0.9690 - val_loss: 0.0887 - val_accuracy: 0.9723\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 2:54 - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 2:02 - loss: 0.4471 - accuracy: 0.9219\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:49 - loss: 0.2990 - accuracy: 0.9479\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:43 - loss: 0.2597 - accuracy: 0.9531\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:41 - loss: 0.2419 - accuracy: 0.9438\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:39 - loss: 0.2037 - accuracy: 0.9531\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:38 - loss: 0.1918 - accuracy: 0.9509\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:36 - loss: 0.1756 - accuracy: 0.9531\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:36 - loss: 0.1790 - accuracy: 0.9514\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:35 - loss: 0.1676 - accuracy: 0.9531\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:34 - loss: 0.1710 - accuracy: 0.9489\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:33 - loss: 0.1640 - accuracy: 0.9505\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:33 - loss: 0.1552 - accuracy: 0.9519\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:33 - loss: 0.1450 - accuracy: 0.9554\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:32 - loss: 0.1361 - accuracy: 0.9583\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:32 - loss: 0.1283 - accuracy: 0.9609\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:32 - loss: 0.1306 - accuracy: 0.9614\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:31 - loss: 0.1303 - accuracy: 0.9583\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:31 - loss: 0.1270 - accuracy: 0.9589\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:30 - loss: 0.1228 - accuracy: 0.9609\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:30 - loss: 0.1224 - accuracy: 0.9613\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:29 - loss: 0.1215 - accuracy: 0.9616\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:28 - loss: 0.1175 - accuracy: 0.9620\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:28 - loss: 0.1138 - accuracy: 0.9622\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:27 - loss: 0.1099 - accuracy: 0.9638\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:27 - loss: 0.1065 - accuracy: 0.9651\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:27 - loss: 0.1104 - accuracy: 0.9653\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:27 - loss: 0.1080 - accuracy: 0.9654\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:28 - loss: 0.1094 - accuracy: 0.9655\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:30 - loss: 0.1058 - accuracy: 0.9667\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.1042 - accuracy: 0.9677\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:29 - loss: 0.1030 - accuracy: 0.9681\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:29 - loss: 0.1004 - accuracy: 0.9691\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:28 - loss: 0.0989 - accuracy: 0.9691\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:28 - loss: 0.0974 - accuracy: 0.9691\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:27 - loss: 0.0951 - accuracy: 0.9700\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:27 - loss: 0.0953 - accuracy: 0.9699\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:26 - loss: 0.0960 - accuracy: 0.9699\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:26 - loss: 0.0943 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:25 - loss: 0.0946 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:24 - loss: 0.0929 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:24 - loss: 0.0938 - accuracy: 0.9705\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:23 - loss: 0.0919 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:23 - loss: 0.0924 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:22 - loss: 0.0921 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:22 - loss: 0.0912 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:21 - loss: 0.0939 - accuracy: 0.9697\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:21 - loss: 0.0927 - accuracy: 0.9697\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:20 - loss: 0.0921 - accuracy: 0.9696\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:20 - loss: 0.0904 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:20 - loss: 0.0887 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:19 - loss: 0.0874 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:19 - loss: 0.0889 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:18 - loss: 0.0875 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:18 - loss: 0.0882 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:17 - loss: 0.0874 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:17 - loss: 0.0908 - accuracy: 0.9701\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:16 - loss: 0.0915 - accuracy: 0.9700\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:17 - loss: 0.0907 - accuracy: 0.9700\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:17 - loss: 0.0906 - accuracy: 0.9695\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:17 - loss: 0.0898 - accuracy: 0.9695\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:17 - loss: 0.0934 - accuracy: 0.9684\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0925 - accuracy: 0.9684\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:16 - loss: 0.0916 - accuracy: 0.9689\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0905 - accuracy: 0.9694\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:15 - loss: 0.0910 - accuracy: 0.9689\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0904 - accuracy: 0.9689\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:15 - loss: 0.0906 - accuracy: 0.9689\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0894 - accuracy: 0.9694\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:14 - loss: 0.0885 - accuracy: 0.9698\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0875 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0873 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0865 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0855 - accuracy: 0.9710\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:11 - loss: 0.0854 - accuracy: 0.9710\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.0852 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:10 - loss: 0.0843 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.0837 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:09 - loss: 0.0827 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.0823 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:09 - loss: 0.0838 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.0833 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.0825 - accuracy: 0.9719\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.0817 - accuracy: 0.9723\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.0821 - accuracy: 0.9722\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:06 - loss: 0.0820 - accuracy: 0.9722\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:06 - loss: 0.0819 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:06 - loss: 0.0811 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:06 - loss: 0.0823 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0832 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0826 - accuracy: 0.9720\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0818 - accuracy: 0.9723\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0815 - accuracy: 0.9723\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:04 - loss: 0.0808 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0800 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:03 - loss: 0.0801 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0794 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:02 - loss: 0.0787 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0810 - accuracy: 0.9727\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:02 - loss: 0.0810 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0802 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:01 - loss: 0.0798 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:00 - loss: 0.0808 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0805 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 59s - loss: 0.0805 - accuracy: 0.9725 \n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 59s - loss: 0.0821 - accuracy: 0.9718\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 59s - loss: 0.0816 - accuracy: 0.9718\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.0810 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 58s - loss: 0.0805 - accuracy: 0.9723\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.0798 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 57s - loss: 0.0797 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 56s - loss: 0.0790 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.0786 - accuracy: 0.9727\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 55s - loss: 0.0789 - accuracy: 0.9724\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 55s - loss: 0.0806 - accuracy: 0.9719\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 55s - loss: 0.0802 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 54s - loss: 0.0797 - accuracy: 0.9723\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 54s - loss: 0.0792 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.0788 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 53s - loss: 0.0782 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.0782 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0789 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 52s - loss: 0.0785 - accuracy: 0.9732\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0795 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 51s - loss: 0.0791 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0791 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.0789 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0784 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0792 - accuracy: 0.9727\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 49s - loss: 0.0787 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0784 - accuracy: 0.9732\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 48s - loss: 0.0783 - accuracy: 0.9731\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0803 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 47s - loss: 0.0809 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 47s - loss: 0.0810 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0810 - accuracy: 0.9721\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.0806 - accuracy: 0.9720\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0801 - accuracy: 0.9722\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 45s - loss: 0.0798 - accuracy: 0.9724\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0803 - accuracy: 0.9724\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 44s - loss: 0.0805 - accuracy: 0.9724\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.0802 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0810 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.0805 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0801 - accuracy: 0.9727\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 42s - loss: 0.0811 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 42s - loss: 0.0810 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0810 - accuracy: 0.9724\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 41s - loss: 0.0806 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0807 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0803 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0801 - accuracy: 0.9727\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0798 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 39s - loss: 0.0795 - accuracy: 0.9731\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0802 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0798 - accuracy: 0.9726\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0796 - accuracy: 0.9728\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0792 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.0790 - accuracy: 0.9730\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0786 - accuracy: 0.9731\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 36s - loss: 0.0787 - accuracy: 0.9731\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0790 - accuracy: 0.9729\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.0806 - accuracy: 0.9725\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0819 - accuracy: 0.9719\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0826 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 34s - loss: 0.0827 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0824 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 33s - loss: 0.0821 - accuracy: 0.9718\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0823 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 33s - loss: 0.0821 - accuracy: 0.9718\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0824 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0825 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 31s - loss: 0.0828 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0827 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 30s - loss: 0.0830 - accuracy: 0.9710\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0829 - accuracy: 0.9710\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0827 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 29s - loss: 0.0823 - accuracy: 0.9711\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0841 - accuracy: 0.9711\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.0837 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0833 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0830 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 27s - loss: 0.0828 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0831 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.0827 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0826 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0822 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0820 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0829 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 24s - loss: 0.0826 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0831 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.0828 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0831 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0830 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0827 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0823 - accuracy: 0.9717\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 21s - loss: 0.0828 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0843 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0840 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0836 - accuracy: 0.9716\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0834 - accuracy: 0.9715\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.0833 - accuracy: 0.9714\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0833 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.0836 - accuracy: 0.9711\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0834 - accuracy: 0.9713\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0838 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.0836 - accuracy: 0.9712\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0838 - accuracy: 0.9711\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.0843 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0843 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0843 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0841 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0844 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.0844 - accuracy: 0.9709\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0843 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0840 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0842 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0840 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0838 - accuracy: 0.9708\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0881 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0877 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0880 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0878 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0884 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0885 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 9s - loss: 0.0881 - accuracy: 0.9705 \n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0878 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0877 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0874 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0877 - accuracy: 0.9707\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0881 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0879 - accuracy: 0.9706\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0885 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0888 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0886 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0883 - accuracy: 0.9705\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0882 - accuracy: 0.9705\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.0884 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0882 - accuracy: 0.9705\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0887 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0887 - accuracy: 0.9704\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0890 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0914 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0918 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0914 - accuracy: 0.9703\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0916 - accuracy: 0.9701\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0914 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9702\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9701\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 145s 581ms/step - loss: 0.0914 - accuracy: 0.9702 - val_loss: 0.0602 - val_accuracy: 0.9808\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:49 - loss: 0.1378 - accuracy: 0.9375\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 2:21 - loss: 0.0844 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 2:30 - loss: 0.0788 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 2:28 - loss: 0.0684 - accuracy: 0.9766\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 2:30 - loss: 0.0627 - accuracy: 0.9750\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 2:18 - loss: 0.0600 - accuracy: 0.9740\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 2:11 - loss: 0.0586 - accuracy: 0.9732\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 2:04 - loss: 0.0521 - accuracy: 0.9766\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 2:00 - loss: 0.0480 - accuracy: 0.9792\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:56 - loss: 0.0536 - accuracy: 0.9750\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:53 - loss: 0.0703 - accuracy: 0.9716\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:51 - loss: 0.0771 - accuracy: 0.9714\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:49 - loss: 0.0722 - accuracy: 0.9736\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:47 - loss: 0.0674 - accuracy: 0.9754\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:46 - loss: 0.0874 - accuracy: 0.9708\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:44 - loss: 0.0877 - accuracy: 0.9707\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:43 - loss: 0.0861 - accuracy: 0.9706\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:41 - loss: 0.0820 - accuracy: 0.9722\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:40 - loss: 0.0830 - accuracy: 0.9704\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:39 - loss: 0.0875 - accuracy: 0.9656\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:38 - loss: 0.0907 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:37 - loss: 0.0884 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:37 - loss: 0.0912 - accuracy: 0.9647\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:36 - loss: 0.0905 - accuracy: 0.9648\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:35 - loss: 0.1053 - accuracy: 0.9638\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:34 - loss: 0.1154 - accuracy: 0.9603\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:33 - loss: 0.1143 - accuracy: 0.9606\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:33 - loss: 0.1112 - accuracy: 0.9621\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:32 - loss: 0.1087 - accuracy: 0.9623\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:31 - loss: 0.1062 - accuracy: 0.9635\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.1046 - accuracy: 0.9637\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:32 - loss: 0.1135 - accuracy: 0.9619\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:33 - loss: 0.1110 - accuracy: 0.9631\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:34 - loss: 0.1112 - accuracy: 0.9632\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:34 - loss: 0.1099 - accuracy: 0.9634\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:33 - loss: 0.1076 - accuracy: 0.9644\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:32 - loss: 0.1072 - accuracy: 0.9637\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:31 - loss: 0.1065 - accuracy: 0.9638\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:31 - loss: 0.1044 - accuracy: 0.9647\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:30 - loss: 0.1071 - accuracy: 0.9633\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:29 - loss: 0.1083 - accuracy: 0.9627\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:29 - loss: 0.1074 - accuracy: 0.9628\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:28 - loss: 0.1076 - accuracy: 0.9622\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:27 - loss: 0.1054 - accuracy: 0.9631\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:27 - loss: 0.1057 - accuracy: 0.9632\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:26 - loss: 0.1051 - accuracy: 0.9626\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:26 - loss: 0.1061 - accuracy: 0.9628\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:25 - loss: 0.1044 - accuracy: 0.9635\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:24 - loss: 0.1024 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:24 - loss: 0.1007 - accuracy: 0.9650\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:23 - loss: 0.0994 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:23 - loss: 0.0997 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:22 - loss: 0.0996 - accuracy: 0.9652\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:22 - loss: 0.0979 - accuracy: 0.9659\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:21 - loss: 0.0986 - accuracy: 0.9659\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:21 - loss: 0.0992 - accuracy: 0.9660\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:20 - loss: 0.0990 - accuracy: 0.9660\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:19 - loss: 0.0991 - accuracy: 0.9661\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:19 - loss: 0.0979 - accuracy: 0.9666\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:18 - loss: 0.0997 - accuracy: 0.9656\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.1014 - accuracy: 0.9652\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:18 - loss: 0.1008 - accuracy: 0.9652\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:19 - loss: 0.1007 - accuracy: 0.9653\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:19 - loss: 0.1027 - accuracy: 0.9639\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:19 - loss: 0.1016 - accuracy: 0.9639\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:18 - loss: 0.1001 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:17 - loss: 0.1000 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:17 - loss: 0.0992 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:16 - loss: 0.1010 - accuracy: 0.9642\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:16 - loss: 0.1022 - accuracy: 0.9638\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:15 - loss: 0.1012 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:15 - loss: 0.0998 - accuracy: 0.9648\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:14 - loss: 0.0998 - accuracy: 0.9649\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:14 - loss: 0.0998 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:13 - loss: 0.1006 - accuracy: 0.9642\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:13 - loss: 0.0995 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:12 - loss: 0.0984 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:12 - loss: 0.0990 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:11 - loss: 0.0990 - accuracy: 0.9644\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:11 - loss: 0.0985 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:10 - loss: 0.0976 - accuracy: 0.9649\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:10 - loss: 0.0983 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:09 - loss: 0.0985 - accuracy: 0.9642\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:09 - loss: 0.0975 - accuracy: 0.9647\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:08 - loss: 0.0976 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:08 - loss: 0.0971 - accuracy: 0.9640\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.0964 - accuracy: 0.9644\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:07 - loss: 0.0958 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:06 - loss: 0.0949 - accuracy: 0.9649\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0944 - accuracy: 0.9653\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0937 - accuracy: 0.9657\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:06 - loss: 0.0928 - accuracy: 0.9660\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:06 - loss: 0.0954 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:06 - loss: 0.0947 - accuracy: 0.9654\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:05 - loss: 0.0957 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:05 - loss: 0.1006 - accuracy: 0.9642\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:04 - loss: 0.1004 - accuracy: 0.9639\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:04 - loss: 0.1015 - accuracy: 0.9640\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:03 - loss: 0.1024 - accuracy: 0.9637\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:03 - loss: 0.1015 - accuracy: 0.9641\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:02 - loss: 0.1022 - accuracy: 0.9635\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:02 - loss: 0.1013 - accuracy: 0.9638\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:01 - loss: 0.1028 - accuracy: 0.9636\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:01 - loss: 0.1019 - accuracy: 0.9639\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 1:00 - loss: 0.1012 - accuracy: 0.9640\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 1:00 - loss: 0.1022 - accuracy: 0.9640\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 59s - loss: 0.1015 - accuracy: 0.9641 \n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 59s - loss: 0.1006 - accuracy: 0.9644\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 59s - loss: 0.1003 - accuracy: 0.9644\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 58s - loss: 0.1011 - accuracy: 0.9645\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 58s - loss: 0.1006 - accuracy: 0.9648\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 57s - loss: 0.1022 - accuracy: 0.9643\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 57s - loss: 0.1014 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 56s - loss: 0.1012 - accuracy: 0.9646\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 56s - loss: 0.1004 - accuracy: 0.9649\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 55s - loss: 0.0997 - accuracy: 0.9652\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 55s - loss: 0.1002 - accuracy: 0.9653\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 54s - loss: 0.1003 - accuracy: 0.9653\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.1007 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 54s - loss: 0.1009 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.1006 - accuracy: 0.9651\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.1000 - accuracy: 0.9654\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 53s - loss: 0.0993 - accuracy: 0.9657\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 53s - loss: 0.1001 - accuracy: 0.9657\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 52s - loss: 0.1002 - accuracy: 0.9657\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 52s - loss: 0.0998 - accuracy: 0.9655\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.1003 - accuracy: 0.9656\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 51s - loss: 0.1002 - accuracy: 0.9658\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0995 - accuracy: 0.9661\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 50s - loss: 0.0993 - accuracy: 0.9661\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0988 - accuracy: 0.9664\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 49s - loss: 0.0982 - accuracy: 0.9666\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 49s - loss: 0.0980 - accuracy: 0.9666\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 48s - loss: 0.0980 - accuracy: 0.9662\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 48s - loss: 0.0982 - accuracy: 0.9657\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0980 - accuracy: 0.9658\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 47s - loss: 0.0975 - accuracy: 0.9660\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0975 - accuracy: 0.9660\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 46s - loss: 0.0968 - accuracy: 0.9663\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0963 - accuracy: 0.9665\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 45s - loss: 0.0965 - accuracy: 0.9665\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 45s - loss: 0.0964 - accuracy: 0.9665\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0958 - accuracy: 0.9668\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 44s - loss: 0.0952 - accuracy: 0.9670\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0945 - accuracy: 0.9672\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 43s - loss: 0.0950 - accuracy: 0.9673\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 42s - loss: 0.0953 - accuracy: 0.9670\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0947 - accuracy: 0.9673\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 41s - loss: 0.0943 - accuracy: 0.9675\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0940 - accuracy: 0.9677\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0936 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0931 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0925 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 40s - loss: 0.0922 - accuracy: 0.9685\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0932 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0941 - accuracy: 0.9677\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 39s - loss: 0.0937 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0933 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 38s - loss: 0.0930 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0937 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 37s - loss: 0.0939 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0935 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.0933 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0934 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0934 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 35s - loss: 0.0931 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0928 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 34s - loss: 0.0931 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0930 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 33s - loss: 0.0944 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0946 - accuracy: 0.9678\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0948 - accuracy: 0.9678\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 32s - loss: 0.0944 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0942 - accuracy: 0.9680\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 31s - loss: 0.0938 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0934 - accuracy: 0.9684\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0929 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 29s - loss: 0.0925 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0937 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.0937 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0940 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0937 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 28s - loss: 0.0935 - accuracy: 0.9682\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0937 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.0934 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0942 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0941 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0947 - accuracy: 0.9679\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0944 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 25s - loss: 0.0950 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0946 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.0945 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0950 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0953 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0956 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0952 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 22s - loss: 0.0949 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0947 - accuracy: 0.9684\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0948 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0944 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0945 - accuracy: 0.9681\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.0941 - accuracy: 0.9683\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0938 - accuracy: 0.9684\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.0935 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0933 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0932 - accuracy: 0.9684\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.0929 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0926 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.0925 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0923 - accuracy: 0.9685\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0922 - accuracy: 0.9685\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0919 - accuracy: 0.9686\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0916 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.0913 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0913 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0910 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0911 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0910 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0907 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0907 - accuracy: 0.9688\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0904 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0902 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0900 - accuracy: 0.9690\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0899 - accuracy: 0.9690\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0904 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 10s - loss: 0.0903 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0903 - accuracy: 0.9688 \n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0901 - accuracy: 0.9689\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0897 - accuracy: 0.9691\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0894 - accuracy: 0.9692\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0891 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0890 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0894 - accuracy: 0.9692\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0890 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0889 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0892 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0895 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.0899 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0900 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0896 - accuracy: 0.9695\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0899 - accuracy: 0.9693\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0896 - accuracy: 0.9694\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0894 - accuracy: 0.9694\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0890 - accuracy: 0.9696\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0888 - accuracy: 0.9697\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0886 - accuracy: 0.9698\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0882 - accuracy: 0.9699\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9701\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9699\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 128s 512ms/step - loss: 0.0880 - accuracy: 0.9699 - val_loss: 0.0333 - val_accuracy: 0.9894\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:45 - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:33 - loss: 0.0644 - accuracy: 0.9844\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:34 - loss: 0.0839 - accuracy: 0.9792\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:35 - loss: 0.0829 - accuracy: 0.9766\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:34 - loss: 0.0849 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:34 - loss: 0.0780 - accuracy: 0.9792\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:35 - loss: 0.0814 - accuracy: 0.9777\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:34 - loss: 0.0794 - accuracy: 0.9766\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:33 - loss: 0.0754 - accuracy: 0.9757\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:33 - loss: 0.0798 - accuracy: 0.9719\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:32 - loss: 0.1019 - accuracy: 0.9659\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:32 - loss: 0.0946 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:31 - loss: 0.1029 - accuracy: 0.9663\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:31 - loss: 0.1119 - accuracy: 0.9643\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:33 - loss: 0.1060 - accuracy: 0.9667\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:36 - loss: 0.1000 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:39 - loss: 0.0955 - accuracy: 0.9706\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:41 - loss: 0.1021 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:41 - loss: 0.0997 - accuracy: 0.9704\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:40 - loss: 0.1032 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:39 - loss: 0.0988 - accuracy: 0.9702\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:38 - loss: 0.1006 - accuracy: 0.9673\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:37 - loss: 0.0964 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:36 - loss: 0.0958 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:35 - loss: 0.0958 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:34 - loss: 0.0922 - accuracy: 0.9700\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:33 - loss: 0.0950 - accuracy: 0.9688\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:33 - loss: 0.0925 - accuracy: 0.9699\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:32 - loss: 0.0897 - accuracy: 0.9709\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:31 - loss: 0.0873 - accuracy: 0.9719\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.0847 - accuracy: 0.9728\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:30 - loss: 0.0831 - accuracy: 0.9736\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:30 - loss: 0.0849 - accuracy: 0.9725\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:29 - loss: 0.0830 - accuracy: 0.9733\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:28 - loss: 0.0807 - accuracy: 0.9741\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:28 - loss: 0.0785 - accuracy: 0.9748\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:26 - loss: 0.0794 - accuracy: 0.9742\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:25 - loss: 0.0775 - accuracy: 0.9749\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:25 - loss: 0.0788 - accuracy: 0.9748\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:24 - loss: 0.0809 - accuracy: 0.9746\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:24 - loss: 0.0829 - accuracy: 0.9745\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:23 - loss: 0.0831 - accuracy: 0.9736\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:23 - loss: 0.0837 - accuracy: 0.9735\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:22 - loss: 0.0852 - accuracy: 0.9726\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:22 - loss: 0.0878 - accuracy: 0.9718\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:23 - loss: 0.0864 - accuracy: 0.9725\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:23 - loss: 0.0847 - accuracy: 0.9730\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:24 - loss: 0.0872 - accuracy: 0.9723\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:24 - loss: 0.0879 - accuracy: 0.9716\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:23 - loss: 0.0915 - accuracy: 0.9703\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:23 - loss: 0.0961 - accuracy: 0.9702\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:22 - loss: 0.0948 - accuracy: 0.9708\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:22 - loss: 0.0963 - accuracy: 0.9708\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:21 - loss: 0.0947 - accuracy: 0.9713\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:21 - loss: 0.0932 - accuracy: 0.9718\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:20 - loss: 0.0921 - accuracy: 0.9718\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:20 - loss: 0.0914 - accuracy: 0.9717\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:19 - loss: 0.0943 - accuracy: 0.9717\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:19 - loss: 0.0956 - accuracy: 0.9711\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:18 - loss: 0.0943 - accuracy: 0.9716\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.0939 - accuracy: 0.9715\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:17 - loss: 0.0928 - accuracy: 0.9720\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0918 - accuracy: 0.9724\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:16 - loss: 0.0916 - accuracy: 0.9724\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0905 - accuracy: 0.9728\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:15 - loss: 0.0907 - accuracy: 0.9723\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0898 - accuracy: 0.9722\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:14 - loss: 0.0901 - accuracy: 0.9722\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0890 - accuracy: 0.9726\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:14 - loss: 0.0878 - accuracy: 0.9730\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0869 - accuracy: 0.9734\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0881 - accuracy: 0.9733\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0879 - accuracy: 0.9732\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0869 - accuracy: 0.9736\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:12 - loss: 0.0858 - accuracy: 0.9739\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:12 - loss: 0.0849 - accuracy: 0.9743\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:12 - loss: 0.0849 - accuracy: 0.9742\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:12 - loss: 0.0840 - accuracy: 0.9746\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:11 - loss: 0.0852 - accuracy: 0.9741\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:11 - loss: 0.0877 - accuracy: 0.9736\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:10 - loss: 0.0916 - accuracy: 0.9732\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:10 - loss: 0.0921 - accuracy: 0.9731\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:09 - loss: 0.0925 - accuracy: 0.9731\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:09 - loss: 0.0918 - accuracy: 0.9730\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:08 - loss: 0.0914 - accuracy: 0.9726\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:08 - loss: 0.0925 - accuracy: 0.9722\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.0932 - accuracy: 0.9721\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:07 - loss: 0.0925 - accuracy: 0.9725\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:07 - loss: 0.0915 - accuracy: 0.9728\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0923 - accuracy: 0.9724\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0915 - accuracy: 0.9727\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0917 - accuracy: 0.9726\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0915 - accuracy: 0.9726\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:04 - loss: 0.0908 - accuracy: 0.9729\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0901 - accuracy: 0.9732\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:03 - loss: 0.0893 - accuracy: 0.9735\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0895 - accuracy: 0.9734\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:02 - loss: 0.0902 - accuracy: 0.9727\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0894 - accuracy: 0.9730\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:02 - loss: 0.0887 - accuracy: 0.9733\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0887 - accuracy: 0.9732\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:01 - loss: 0.0882 - accuracy: 0.9735\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:00 - loss: 0.0877 - accuracy: 0.9737\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0870 - accuracy: 0.9740\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 1:00 - loss: 0.0865 - accuracy: 0.9743\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 1:00 - loss: 0.0858 - accuracy: 0.9745\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 1:00 - loss: 0.0853 - accuracy: 0.9747\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 59s - loss: 0.0848 - accuracy: 0.9750 \n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 59s - loss: 0.0844 - accuracy: 0.9749\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 58s - loss: 0.0838 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 58s - loss: 0.0832 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 57s - loss: 0.0825 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 57s - loss: 0.0829 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 56s - loss: 0.0829 - accuracy: 0.9749\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 56s - loss: 0.0825 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 56s - loss: 0.0819 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 55s - loss: 0.0841 - accuracy: 0.9748\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 55s - loss: 0.0835 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.0834 - accuracy: 0.9749\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 54s - loss: 0.0828 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.0822 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0816 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 52s - loss: 0.0812 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0810 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 52s - loss: 0.0804 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0834 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.0849 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0846 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0840 - accuracy: 0.9757\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 49s - loss: 0.0835 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0835 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 48s - loss: 0.0835 - accuracy: 0.9757\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0858 - accuracy: 0.9757\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 48s - loss: 0.0854 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 48s - loss: 0.0850 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0849 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 47s - loss: 0.0856 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 47s - loss: 0.0852 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 46s - loss: 0.0846 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 46s - loss: 0.0840 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 45s - loss: 0.0835 - accuracy: 0.9760\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 45s - loss: 0.0831 - accuracy: 0.9761\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0825 - accuracy: 0.9763\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 44s - loss: 0.0823 - accuracy: 0.9762\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0850 - accuracy: 0.9760\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 43s - loss: 0.0849 - accuracy: 0.9759\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 43s - loss: 0.0844 - accuracy: 0.9761\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0840 - accuracy: 0.9763\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 42s - loss: 0.0840 - accuracy: 0.9762\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0835 - accuracy: 0.9764\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0833 - accuracy: 0.9763\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0837 - accuracy: 0.9763\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0833 - accuracy: 0.9764\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 40s - loss: 0.0831 - accuracy: 0.9764\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0839 - accuracy: 0.9763\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0846 - accuracy: 0.9761\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0860 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0859 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.0855 - accuracy: 0.9759\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0850 - accuracy: 0.9761\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 37s - loss: 0.0856 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0854 - accuracy: 0.9758\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.0851 - accuracy: 0.9759\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0851 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0849 - accuracy: 0.9757\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 35s - loss: 0.0847 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0857 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 34s - loss: 0.0855 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 34s - loss: 0.0852 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 33s - loss: 0.0851 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 33s - loss: 0.0849 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0844 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 32s - loss: 0.0839 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0835 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 31s - loss: 0.0834 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0839 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0842 - accuracy: 0.9748\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 30s - loss: 0.0838 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0835 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.0831 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0826 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0827 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 27s - loss: 0.0823 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0821 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.0818 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0828 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0825 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0822 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0818 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 24s - loss: 0.0818 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0821 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.0821 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0825 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0824 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 23s - loss: 0.0822 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0822 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 22s - loss: 0.0819 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0827 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0824 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0821 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0825 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 20s - loss: 0.0823 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0827 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.0826 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0822 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0819 - accuracy: 0.9755\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.0816 - accuracy: 0.9756\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0823 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.0829 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0827 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0826 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0824 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0821 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.0823 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0829 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0832 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0833 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0835 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0833 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0832 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0832 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0830 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0830 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0827 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0829 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 10s - loss: 0.0827 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0824 - accuracy: 0.9754 \n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0822 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0824 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0823 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0825 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0824 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0820 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0833 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0832 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0831 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0829 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 5s - loss: 0.0826 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0823 - accuracy: 0.9754\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0821 - accuracy: 0.9753\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0822 - accuracy: 0.9750\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0819 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0817 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0816 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0816 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0815 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0814 - accuracy: 0.9751\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9752\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 0.0811 - accuracy: 0.9752 - val_loss: 0.0338 - val_accuracy: 0.9879\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:46 - loss: 0.1908 - accuracy: 0.9375\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:32 - loss: 0.0981 - accuracy: 0.9688\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:33 - loss: 0.0677 - accuracy: 0.9792\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:33 - loss: 0.0753 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:33 - loss: 0.0683 - accuracy: 0.9812\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:33 - loss: 0.0798 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:34 - loss: 0.0755 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:33 - loss: 0.0671 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:33 - loss: 0.0822 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:33 - loss: 0.0744 - accuracy: 0.9750\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:32 - loss: 0.0727 - accuracy: 0.9744\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:32 - loss: 0.0671 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:32 - loss: 0.0626 - accuracy: 0.9784\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:32 - loss: 0.0627 - accuracy: 0.9777\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:32 - loss: 0.0623 - accuracy: 0.9771\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:32 - loss: 0.0626 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:31 - loss: 0.0597 - accuracy: 0.9779\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:31 - loss: 0.0587 - accuracy: 0.9774\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:30 - loss: 0.0568 - accuracy: 0.9786\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:30 - loss: 0.0547 - accuracy: 0.9797\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:29 - loss: 0.0585 - accuracy: 0.9792\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:29 - loss: 0.0579 - accuracy: 0.9801\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:28 - loss: 0.0561 - accuracy: 0.9810\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:28 - loss: 0.0542 - accuracy: 0.9818\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:28 - loss: 0.0530 - accuracy: 0.9825\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:29 - loss: 0.0527 - accuracy: 0.9832\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:31 - loss: 0.0511 - accuracy: 0.9838\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:32 - loss: 0.0494 - accuracy: 0.9844\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:33 - loss: 0.0507 - accuracy: 0.9828\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:32 - loss: 0.0519 - accuracy: 0.9823\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.0556 - accuracy: 0.9819\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:31 - loss: 0.0541 - accuracy: 0.9824\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:30 - loss: 0.0546 - accuracy: 0.9811\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:28 - loss: 0.0575 - accuracy: 0.9803\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:28 - loss: 0.0560 - accuracy: 0.9809\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:27 - loss: 0.0546 - accuracy: 0.9814\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:27 - loss: 0.0574 - accuracy: 0.9811\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:26 - loss: 0.0601 - accuracy: 0.9791\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:25 - loss: 0.0590 - accuracy: 0.9796\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:25 - loss: 0.0582 - accuracy: 0.9794\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:24 - loss: 0.0575 - accuracy: 0.9791\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:24 - loss: 0.0649 - accuracy: 0.9773\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:23 - loss: 0.0678 - accuracy: 0.9764\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:23 - loss: 0.0667 - accuracy: 0.9769\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:23 - loss: 0.0664 - accuracy: 0.9768\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:22 - loss: 0.0656 - accuracy: 0.9773\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:22 - loss: 0.0648 - accuracy: 0.9778\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:21 - loss: 0.0640 - accuracy: 0.9782\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:21 - loss: 0.0684 - accuracy: 0.9767\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:20 - loss: 0.0690 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:20 - loss: 0.0707 - accuracy: 0.9758\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:19 - loss: 0.0714 - accuracy: 0.9757\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:19 - loss: 0.0703 - accuracy: 0.9761\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:19 - loss: 0.0727 - accuracy: 0.9754\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:18 - loss: 0.0731 - accuracy: 0.9741\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:18 - loss: 0.0731 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:19 - loss: 0.0719 - accuracy: 0.9745\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:19 - loss: 0.0707 - accuracy: 0.9749\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:19 - loss: 0.0695 - accuracy: 0.9754\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:19 - loss: 0.0699 - accuracy: 0.9747\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.0689 - accuracy: 0.9752\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:18 - loss: 0.0679 - accuracy: 0.9756\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0671 - accuracy: 0.9760\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:17 - loss: 0.0661 - accuracy: 0.9763\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0655 - accuracy: 0.9762\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:16 - loss: 0.0649 - accuracy: 0.9766\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0656 - accuracy: 0.9765\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:15 - loss: 0.0661 - accuracy: 0.9763\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0701 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:14 - loss: 0.0718 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0715 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0717 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0728 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0726 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:11 - loss: 0.0721 - accuracy: 0.9727\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.0713 - accuracy: 0.9731\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:11 - loss: 0.0709 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.0706 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:10 - loss: 0.0702 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.0694 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:09 - loss: 0.0688 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.0692 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.0691 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.0685 - accuracy: 0.9741\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.0688 - accuracy: 0.9741\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:07 - loss: 0.0688 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.0705 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:07 - loss: 0.0698 - accuracy: 0.9742\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:07 - loss: 0.0714 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0721 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0723 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0730 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0731 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:05 - loss: 0.0741 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0739 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:04 - loss: 0.0736 - accuracy: 0.9721\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0729 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:03 - loss: 0.0731 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0725 - accuracy: 0.9727\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:02 - loss: 0.0726 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0725 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:01 - loss: 0.0727 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:00 - loss: 0.0722 - accuracy: 0.9725\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0723 - accuracy: 0.9725\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 1:00 - loss: 0.0718 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 59s - loss: 0.0714 - accuracy: 0.9730 \n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 59s - loss: 0.0708 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.0716 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 58s - loss: 0.0729 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.0724 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 57s - loss: 0.0729 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 56s - loss: 0.0727 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.0724 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 56s - loss: 0.0719 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 55s - loss: 0.0718 - accuracy: 0.9721\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 55s - loss: 0.0722 - accuracy: 0.9721\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 55s - loss: 0.0729 - accuracy: 0.9718\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 55s - loss: 0.0735 - accuracy: 0.9715\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.0733 - accuracy: 0.9715\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 54s - loss: 0.0728 - accuracy: 0.9717\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.0727 - accuracy: 0.9717\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0729 - accuracy: 0.9717\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 53s - loss: 0.0736 - accuracy: 0.9717\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0738 - accuracy: 0.9716\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 52s - loss: 0.0747 - accuracy: 0.9716\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0741 - accuracy: 0.9718\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.0738 - accuracy: 0.9721\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0733 - accuracy: 0.9723\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0740 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 49s - loss: 0.0741 - accuracy: 0.9717\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0738 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 49s - loss: 0.0734 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0731 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 48s - loss: 0.0727 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 47s - loss: 0.0727 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0763 - accuracy: 0.9721\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.0762 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0762 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 46s - loss: 0.0760 - accuracy: 0.9720\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0756 - accuracy: 0.9722\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 45s - loss: 0.0751 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.0748 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0747 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.0744 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0739 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 43s - loss: 0.0738 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 43s - loss: 0.0736 - accuracy: 0.9731\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0736 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 42s - loss: 0.0732 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0738 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0737 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 41s - loss: 0.0748 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0757 - accuracy: 0.9731\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 40s - loss: 0.0766 - accuracy: 0.9727\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0764 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0763 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0777 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0777 - accuracy: 0.9724\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 38s - loss: 0.0775 - accuracy: 0.9726\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0778 - accuracy: 0.9725\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 37s - loss: 0.0775 - accuracy: 0.9727\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0771 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.0769 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0768 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0765 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 35s - loss: 0.0761 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0766 - accuracy: 0.9728\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 34s - loss: 0.0762 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0760 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 33s - loss: 0.0767 - accuracy: 0.9727\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0767 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0763 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 32s - loss: 0.0759 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0756 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 31s - loss: 0.0756 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 31s - loss: 0.0754 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0750 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 30s - loss: 0.0799 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0797 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.0800 - accuracy: 0.9732\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0797 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0793 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 28s - loss: 0.0798 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0794 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.0792 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0791 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0787 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0795 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0792 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 25s - loss: 0.0794 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0790 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.0787 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0783 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0781 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0778 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0775 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 22s - loss: 0.0782 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0780 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0779 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0777 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0777 - accuracy: 0.9740\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 20s - loss: 0.0774 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0773 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.0774 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0773 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0770 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 18s - loss: 0.0771 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0771 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.0768 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0764 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0769 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0768 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0766 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.0768 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0766 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0764 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0766 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0785 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 13s - loss: 0.0783 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0780 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0778 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0776 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0775 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0785 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0782 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 10s - loss: 0.0793 - accuracy: 0.9731\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0791 - accuracy: 0.9731 \n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0793 - accuracy: 0.9729\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0792 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0791 - accuracy: 0.9730\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0788 - accuracy: 0.9731\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0785 - accuracy: 0.9733\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0783 - accuracy: 0.9734\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0782 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0781 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0779 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0776 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 5s - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0776 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0773 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0778 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0774 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0773 - accuracy: 0.9735\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0771 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0771 - accuracy: 0.9736\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0771 - accuracy: 0.9737\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9739\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9738\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 0.0765 - accuracy: 0.9738 - val_loss: 0.0405 - val_accuracy: 0.9879\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 1:47 - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:31 - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:37 - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:37 - loss: 0.0477 - accuracy: 0.9922\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:36 - loss: 0.0622 - accuracy: 0.9875\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:37 - loss: 0.0540 - accuracy: 0.9896\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:36 - loss: 0.0482 - accuracy: 0.9911\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:36 - loss: 0.0426 - accuracy: 0.9922\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:35 - loss: 0.0636 - accuracy: 0.9826\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:34 - loss: 0.0603 - accuracy: 0.9812\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:34 - loss: 0.0618 - accuracy: 0.9773\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:33 - loss: 0.0572 - accuracy: 0.9792\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:33 - loss: 0.0598 - accuracy: 0.9784\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:32 - loss: 0.0617 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:32 - loss: 0.0725 - accuracy: 0.9729\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:31 - loss: 0.0682 - accuracy: 0.9746\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:31 - loss: 0.0653 - accuracy: 0.9761\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:30 - loss: 0.0646 - accuracy: 0.9757\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:30 - loss: 0.0619 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:29 - loss: 0.0626 - accuracy: 0.9766\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:29 - loss: 0.0607 - accuracy: 0.9777\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:28 - loss: 0.0687 - accuracy: 0.9744\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:28 - loss: 0.0696 - accuracy: 0.9742\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:27 - loss: 0.0668 - accuracy: 0.9753\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:27 - loss: 0.0692 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:28 - loss: 0.0764 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:29 - loss: 0.0762 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:31 - loss: 0.0741 - accuracy: 0.9743\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:32 - loss: 0.0724 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:32 - loss: 0.0701 - accuracy: 0.9760\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:31 - loss: 0.0737 - accuracy: 0.9758\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:30 - loss: 0.0738 - accuracy: 0.9756\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:30 - loss: 0.0738 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:29 - loss: 0.0723 - accuracy: 0.9761\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:29 - loss: 0.0704 - accuracy: 0.9768\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:28 - loss: 0.0694 - accuracy: 0.9774\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:28 - loss: 0.0717 - accuracy: 0.9764\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:27 - loss: 0.0700 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:27 - loss: 0.0712 - accuracy: 0.9760\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:26 - loss: 0.0705 - accuracy: 0.9758\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:26 - loss: 0.0720 - accuracy: 0.9756\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:25 - loss: 0.0706 - accuracy: 0.9762\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:25 - loss: 0.0699 - accuracy: 0.9767\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:24 - loss: 0.0695 - accuracy: 0.9766\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:24 - loss: 0.0742 - accuracy: 0.9757\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:23 - loss: 0.0751 - accuracy: 0.9755\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:23 - loss: 0.0764 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:22 - loss: 0.0765 - accuracy: 0.9746\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:22 - loss: 0.0754 - accuracy: 0.9751\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:21 - loss: 0.0753 - accuracy: 0.9756\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:21 - loss: 0.0740 - accuracy: 0.9761\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:20 - loss: 0.0726 - accuracy: 0.9766\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:19 - loss: 0.0722 - accuracy: 0.9767\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:19 - loss: 0.0717 - accuracy: 0.9766\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:18 - loss: 0.0708 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:18 - loss: 0.0697 - accuracy: 0.9774\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:18 - loss: 0.0732 - accuracy: 0.9767\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:19 - loss: 0.0723 - accuracy: 0.9771\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:19 - loss: 0.0711 - accuracy: 0.9775\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:19 - loss: 0.0712 - accuracy: 0.9774\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.0706 - accuracy: 0.9772\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:18 - loss: 0.0702 - accuracy: 0.9776\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0698 - accuracy: 0.9775\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:17 - loss: 0.0691 - accuracy: 0.9778\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0702 - accuracy: 0.9777\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:16 - loss: 0.0691 - accuracy: 0.9780\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0706 - accuracy: 0.9769\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:15 - loss: 0.0705 - accuracy: 0.9768\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0696 - accuracy: 0.9771\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:14 - loss: 0.0692 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0685 - accuracy: 0.9774\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0678 - accuracy: 0.9777\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0671 - accuracy: 0.9780\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0665 - accuracy: 0.9783\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:11 - loss: 0.0657 - accuracy: 0.9786\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.0654 - accuracy: 0.9784\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:11 - loss: 0.0650 - accuracy: 0.9787\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.0651 - accuracy: 0.9786\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:10 - loss: 0.0643 - accuracy: 0.9789\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.0641 - accuracy: 0.9787\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:09 - loss: 0.0637 - accuracy: 0.9790\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.0633 - accuracy: 0.9793\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.0648 - accuracy: 0.9791\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.0657 - accuracy: 0.9786\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.0657 - accuracy: 0.9781\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:07 - loss: 0.0650 - accuracy: 0.9784\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:07 - loss: 0.0650 - accuracy: 0.9779\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:07 - loss: 0.0645 - accuracy: 0.9782\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:07 - loss: 0.0650 - accuracy: 0.9777\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0644 - accuracy: 0.9780\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0638 - accuracy: 0.9782\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0640 - accuracy: 0.9781\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0636 - accuracy: 0.9783\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:05 - loss: 0.0639 - accuracy: 0.9782\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0633 - accuracy: 0.9785\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:04 - loss: 0.0632 - accuracy: 0.9784\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0666 - accuracy: 0.9776\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:03 - loss: 0.0664 - accuracy: 0.9775\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0668 - accuracy: 0.9774\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:02 - loss: 0.0699 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0696 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:01 - loss: 0.0690 - accuracy: 0.9772\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:01 - loss: 0.0694 - accuracy: 0.9771\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0693 - accuracy: 0.9767\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 1:00 - loss: 0.0690 - accuracy: 0.9769\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 59s - loss: 0.0689 - accuracy: 0.9769 \n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 59s - loss: 0.0687 - accuracy: 0.9768\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.0686 - accuracy: 0.9767\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 58s - loss: 0.0695 - accuracy: 0.9766\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.0691 - accuracy: 0.9769\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 57s - loss: 0.0689 - accuracy: 0.9768\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 57s - loss: 0.0685 - accuracy: 0.9770\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.0701 - accuracy: 0.9764\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 56s - loss: 0.0704 - accuracy: 0.9760\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 56s - loss: 0.0701 - accuracy: 0.9760\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 55s - loss: 0.0705 - accuracy: 0.9759\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 55s - loss: 0.0705 - accuracy: 0.9758\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 55s - loss: 0.0699 - accuracy: 0.9760\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 55s - loss: 0.0693 - accuracy: 0.9762\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 54s - loss: 0.0690 - accuracy: 0.9762\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 54s - loss: 0.0691 - accuracy: 0.9761\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0690 - accuracy: 0.9761\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 53s - loss: 0.0691 - accuracy: 0.9757\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0692 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 52s - loss: 0.0706 - accuracy: 0.9751\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0715 - accuracy: 0.9748\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.0710 - accuracy: 0.9750\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0706 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0702 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 50s - loss: 0.0698 - accuracy: 0.9756\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0712 - accuracy: 0.9753\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 49s - loss: 0.0708 - accuracy: 0.9755\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0710 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 48s - loss: 0.0725 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 47s - loss: 0.0724 - accuracy: 0.9751\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0720 - accuracy: 0.9753\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.0716 - accuracy: 0.9755\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0713 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 46s - loss: 0.0711 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0711 - accuracy: 0.9753\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 45s - loss: 0.0711 - accuracy: 0.9753\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.0709 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0704 - accuracy: 0.9754\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.0714 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0716 - accuracy: 0.9749\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 43s - loss: 0.0714 - accuracy: 0.9751\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 43s - loss: 0.0709 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0712 - accuracy: 0.9752\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 42s - loss: 0.0719 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0716 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0716 - accuracy: 0.9746\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0712 - accuracy: 0.9748\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0711 - accuracy: 0.9748\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 40s - loss: 0.0709 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0706 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0705 - accuracy: 0.9747\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0718 - accuracy: 0.9742\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0737 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.0736 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0734 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 37s - loss: 0.0732 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0735 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 36s - loss: 0.0744 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0740 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0737 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 34s - loss: 0.0736 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0732 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 34s - loss: 0.0731 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0727 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 33s - loss: 0.0727 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0724 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0727 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 31s - loss: 0.0723 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0721 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 31s - loss: 0.0721 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0727 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 30s - loss: 0.0750 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 30s - loss: 0.0750 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0749 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 29s - loss: 0.0751 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0755 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0766 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 28s - loss: 0.0770 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0771 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 27s - loss: 0.0767 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0771 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0777 - accuracy: 0.9733\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0787 - accuracy: 0.9728\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0784 - accuracy: 0.9730\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 25s - loss: 0.0783 - accuracy: 0.9729\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0779 - accuracy: 0.9731\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 24s - loss: 0.0787 - accuracy: 0.9731\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0783 - accuracy: 0.9732\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0784 - accuracy: 0.9732\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0781 - accuracy: 0.9733\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0777 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 22s - loss: 0.0774 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0787 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0785 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0785 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0783 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.0791 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0790 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 19s - loss: 0.0787 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0784 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0785 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 18s - loss: 0.0784 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0781 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 17s - loss: 0.0781 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0781 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0781 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0784 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0785 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 15s - loss: 0.0786 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0783 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0784 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0784 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0781 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0780 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0786 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0783 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0781 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0800 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0801 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0799 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 10s - loss: 0.0798 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0802 - accuracy: 0.9735 \n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0800 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0797 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0798 - accuracy: 0.9737\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0796 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0795 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0806 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0805 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0802 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0801 - accuracy: 0.9734\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0798 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 5s - loss: 0.0797 - accuracy: 0.9735\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0795 - accuracy: 0.9736\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0792 - accuracy: 0.9738\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0791 - accuracy: 0.9739\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0788 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0788 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0785 - accuracy: 0.9741\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0785 - accuracy: 0.9740\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0783 - accuracy: 0.9741\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0780 - accuracy: 0.9743\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9742\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9742\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 145s 582ms/step - loss: 0.0781 - accuracy: 0.9743 - val_loss: 0.0435 - val_accuracy: 0.9879\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  1/250 [..............................] - ETA: 2:48 - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  2/250 [..............................] - ETA: 1:55 - loss: 0.0167 - accuracy: 0.9844\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  3/250 [..............................] - ETA: 1:45 - loss: 0.0260 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  4/250 [..............................] - ETA: 1:41 - loss: 0.0397 - accuracy: 0.9766\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  5/250 [..............................] - ETA: 1:39 - loss: 0.0529 - accuracy: 0.9750\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  6/250 [..............................] - ETA: 1:38 - loss: 0.0448 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  7/250 [..............................] - ETA: 1:36 - loss: 0.0400 - accuracy: 0.9821\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  8/250 [..............................] - ETA: 1:36 - loss: 0.0368 - accuracy: 0.9844\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "  9/250 [>.............................] - ETA: 1:35 - loss: 0.0551 - accuracy: 0.9826\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 10/250 [>.............................] - ETA: 1:35 - loss: 0.0570 - accuracy: 0.9812\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 11/250 [>.............................] - ETA: 1:34 - loss: 0.0719 - accuracy: 0.9773\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 12/250 [>.............................] - ETA: 1:33 - loss: 0.0664 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 13/250 [>.............................] - ETA: 1:33 - loss: 0.0632 - accuracy: 0.9808\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 14/250 [>.............................] - ETA: 1:32 - loss: 0.0589 - accuracy: 0.9821\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 15/250 [>.............................] - ETA: 1:31 - loss: 0.0552 - accuracy: 0.9833\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 16/250 [>.............................] - ETA: 1:31 - loss: 0.0588 - accuracy: 0.9824\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 17/250 [=>............................] - ETA: 1:30 - loss: 0.0561 - accuracy: 0.9835\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 18/250 [=>............................] - ETA: 1:30 - loss: 0.0559 - accuracy: 0.9826\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 19/250 [=>............................] - ETA: 1:29 - loss: 0.0560 - accuracy: 0.9819\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 20/250 [=>............................] - ETA: 1:28 - loss: 0.0541 - accuracy: 0.9828\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 21/250 [=>............................] - ETA: 1:28 - loss: 0.0523 - accuracy: 0.9836\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 22/250 [=>............................] - ETA: 1:28 - loss: 0.0654 - accuracy: 0.9815\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 23/250 [=>............................] - ETA: 1:27 - loss: 0.0634 - accuracy: 0.9823\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 24/250 [=>............................] - ETA: 1:27 - loss: 0.0618 - accuracy: 0.9831\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 25/250 [==>...........................] - ETA: 1:26 - loss: 0.0609 - accuracy: 0.9825\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 26/250 [==>...........................] - ETA: 1:26 - loss: 0.0596 - accuracy: 0.9832\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 27/250 [==>...........................] - ETA: 1:26 - loss: 0.0626 - accuracy: 0.9815\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 28/250 [==>...........................] - ETA: 1:26 - loss: 0.0623 - accuracy: 0.9810\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 29/250 [==>...........................] - ETA: 1:27 - loss: 0.0664 - accuracy: 0.9806\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 30/250 [==>...........................] - ETA: 1:29 - loss: 0.0652 - accuracy: 0.9802\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 31/250 [==>...........................] - ETA: 1:30 - loss: 0.0636 - accuracy: 0.9808\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 32/250 [==>...........................] - ETA: 1:31 - loss: 0.0619 - accuracy: 0.9814\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 33/250 [==>...........................] - ETA: 1:30 - loss: 0.0603 - accuracy: 0.9820\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 34/250 [===>..........................] - ETA: 1:30 - loss: 0.0643 - accuracy: 0.9816\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 35/250 [===>..........................] - ETA: 1:29 - loss: 0.0652 - accuracy: 0.9812\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 36/250 [===>..........................] - ETA: 1:29 - loss: 0.0656 - accuracy: 0.9809\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 37/250 [===>..........................] - ETA: 1:28 - loss: 0.0658 - accuracy: 0.9806\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 38/250 [===>..........................] - ETA: 1:28 - loss: 0.0674 - accuracy: 0.9803\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 39/250 [===>..........................] - ETA: 1:27 - loss: 0.0675 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 40/250 [===>..........................] - ETA: 1:26 - loss: 0.0661 - accuracy: 0.9797\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 41/250 [===>..........................] - ETA: 1:26 - loss: 0.0706 - accuracy: 0.9787\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 42/250 [====>.........................] - ETA: 1:25 - loss: 0.0694 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 43/250 [====>.........................] - ETA: 1:25 - loss: 0.0704 - accuracy: 0.9789\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 44/250 [====>.........................] - ETA: 1:24 - loss: 0.0689 - accuracy: 0.9794\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 45/250 [====>.........................] - ETA: 1:24 - loss: 0.0726 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 46/250 [====>.........................] - ETA: 1:23 - loss: 0.0712 - accuracy: 0.9796\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 47/250 [====>.........................] - ETA: 1:23 - loss: 0.0822 - accuracy: 0.9787\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 48/250 [====>.........................] - ETA: 1:22 - loss: 0.0832 - accuracy: 0.9785\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 49/250 [====>.........................] - ETA: 1:22 - loss: 0.0833 - accuracy: 0.9777\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 50/250 [=====>........................] - ETA: 1:21 - loss: 0.0848 - accuracy: 0.9775\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 51/250 [=====>........................] - ETA: 1:21 - loss: 0.0838 - accuracy: 0.9773\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 52/250 [=====>........................] - ETA: 1:20 - loss: 0.0853 - accuracy: 0.9772\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 53/250 [=====>........................] - ETA: 1:20 - loss: 0.0842 - accuracy: 0.9776\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 54/250 [=====>........................] - ETA: 1:19 - loss: 0.0827 - accuracy: 0.9780\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 55/250 [=====>........................] - ETA: 1:19 - loss: 0.0814 - accuracy: 0.9784\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 56/250 [=====>........................] - ETA: 1:18 - loss: 0.0802 - accuracy: 0.9788\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 57/250 [=====>........................] - ETA: 1:18 - loss: 0.0796 - accuracy: 0.9786\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 58/250 [=====>........................] - ETA: 1:18 - loss: 0.0791 - accuracy: 0.9784\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 59/250 [======>.......................] - ETA: 1:18 - loss: 0.0783 - accuracy: 0.9788\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 60/250 [======>.......................] - ETA: 1:18 - loss: 0.0772 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 61/250 [======>.......................] - ETA: 1:18 - loss: 0.0778 - accuracy: 0.9790\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 62/250 [======>.......................] - ETA: 1:18 - loss: 0.0769 - accuracy: 0.9793\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 63/250 [======>.......................] - ETA: 1:17 - loss: 0.0762 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 64/250 [======>.......................] - ETA: 1:17 - loss: 0.0769 - accuracy: 0.9790\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 65/250 [======>.......................] - ETA: 1:16 - loss: 0.0769 - accuracy: 0.9788\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 66/250 [======>.......................] - ETA: 1:16 - loss: 0.0760 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 67/250 [=======>......................] - ETA: 1:15 - loss: 0.0760 - accuracy: 0.9790\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 68/250 [=======>......................] - ETA: 1:15 - loss: 0.0768 - accuracy: 0.9789\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 69/250 [=======>......................] - ETA: 1:14 - loss: 0.0759 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 70/250 [=======>......................] - ETA: 1:14 - loss: 0.0760 - accuracy: 0.9790\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 71/250 [=======>......................] - ETA: 1:13 - loss: 0.0768 - accuracy: 0.9789\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 72/250 [=======>......................] - ETA: 1:13 - loss: 0.0802 - accuracy: 0.9787\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 73/250 [=======>......................] - ETA: 1:12 - loss: 0.0792 - accuracy: 0.9790\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 74/250 [=======>......................] - ETA: 1:12 - loss: 0.0786 - accuracy: 0.9793\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 75/250 [========>.....................] - ETA: 1:12 - loss: 0.0786 - accuracy: 0.9792\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 76/250 [========>.....................] - ETA: 1:11 - loss: 0.0784 - accuracy: 0.9786\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 77/250 [========>.....................] - ETA: 1:11 - loss: 0.0781 - accuracy: 0.9785\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 78/250 [========>.....................] - ETA: 1:10 - loss: 0.0798 - accuracy: 0.9776\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 79/250 [========>.....................] - ETA: 1:10 - loss: 0.0803 - accuracy: 0.9771\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 80/250 [========>.....................] - ETA: 1:09 - loss: 0.0818 - accuracy: 0.9762\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 81/250 [========>.....................] - ETA: 1:09 - loss: 0.0854 - accuracy: 0.9757\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 82/250 [========>.....................] - ETA: 1:08 - loss: 0.0844 - accuracy: 0.9760\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 83/250 [========>.....................] - ETA: 1:08 - loss: 0.0835 - accuracy: 0.9763\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 84/250 [=========>....................] - ETA: 1:07 - loss: 0.0838 - accuracy: 0.9758\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 85/250 [=========>....................] - ETA: 1:07 - loss: 0.0829 - accuracy: 0.9761\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 86/250 [=========>....................] - ETA: 1:06 - loss: 0.0821 - accuracy: 0.9764\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 87/250 [=========>....................] - ETA: 1:06 - loss: 0.0815 - accuracy: 0.9767\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 88/250 [=========>....................] - ETA: 1:06 - loss: 0.0819 - accuracy: 0.9762\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 89/250 [=========>....................] - ETA: 1:06 - loss: 0.0847 - accuracy: 0.9754\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 90/250 [=========>....................] - ETA: 1:06 - loss: 0.0851 - accuracy: 0.9753\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 91/250 [=========>....................] - ETA: 1:06 - loss: 0.0864 - accuracy: 0.9749\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 92/250 [==========>...................] - ETA: 1:05 - loss: 0.0858 - accuracy: 0.9752\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 93/250 [==========>...................] - ETA: 1:05 - loss: 0.0854 - accuracy: 0.9751\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 94/250 [==========>...................] - ETA: 1:04 - loss: 0.0848 - accuracy: 0.9751\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 95/250 [==========>...................] - ETA: 1:04 - loss: 0.0840 - accuracy: 0.9753\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 96/250 [==========>...................] - ETA: 1:03 - loss: 0.0838 - accuracy: 0.9753\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 97/250 [==========>...................] - ETA: 1:03 - loss: 0.0851 - accuracy: 0.9749\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 98/250 [==========>...................] - ETA: 1:03 - loss: 0.0846 - accuracy: 0.9748\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      " 99/250 [==========>...................] - ETA: 1:02 - loss: 0.0887 - accuracy: 0.9747\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "100/250 [===========>..................] - ETA: 1:02 - loss: 0.0878 - accuracy: 0.9750\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "101/250 [===========>..................] - ETA: 1:01 - loss: 0.0875 - accuracy: 0.9749\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "102/250 [===========>..................] - ETA: 1:01 - loss: 0.0885 - accuracy: 0.9746\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "103/250 [===========>..................] - ETA: 1:00 - loss: 0.0912 - accuracy: 0.9742\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "104/250 [===========>..................] - ETA: 1:00 - loss: 0.0922 - accuracy: 0.9736\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "105/250 [===========>..................] - ETA: 59s - loss: 0.0918 - accuracy: 0.9735 \n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "106/250 [===========>..................] - ETA: 59s - loss: 0.0933 - accuracy: 0.9735\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "107/250 [===========>..................] - ETA: 58s - loss: 0.0930 - accuracy: 0.9734\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "108/250 [===========>..................] - ETA: 58s - loss: 0.0925 - accuracy: 0.9734\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "109/250 [============>.................] - ETA: 58s - loss: 0.0917 - accuracy: 0.9736\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "110/250 [============>.................] - ETA: 57s - loss: 0.0916 - accuracy: 0.9736\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "111/250 [============>.................] - ETA: 57s - loss: 0.0952 - accuracy: 0.9730\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "112/250 [============>.................] - ETA: 56s - loss: 0.0951 - accuracy: 0.9729\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "113/250 [============>.................] - ETA: 56s - loss: 0.0972 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "114/250 [============>.................] - ETA: 55s - loss: 0.0964 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "115/250 [============>.................] - ETA: 55s - loss: 0.0960 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "116/250 [============>.................] - ETA: 54s - loss: 0.0963 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "117/250 [=============>................] - ETA: 54s - loss: 0.0970 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "118/250 [=============>................] - ETA: 54s - loss: 0.0969 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "119/250 [=============>................] - ETA: 54s - loss: 0.0963 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "120/250 [=============>................] - ETA: 53s - loss: 0.0965 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "121/250 [=============>................] - ETA: 53s - loss: 0.0960 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "122/250 [=============>................] - ETA: 53s - loss: 0.0956 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "123/250 [=============>................] - ETA: 52s - loss: 0.0953 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "124/250 [=============>................] - ETA: 52s - loss: 0.0967 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "125/250 [==============>...............] - ETA: 51s - loss: 0.0960 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "126/250 [==============>...............] - ETA: 51s - loss: 0.0975 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "127/250 [==============>...............] - ETA: 51s - loss: 0.0969 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "128/250 [==============>...............] - ETA: 50s - loss: 0.0962 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "129/250 [==============>...............] - ETA: 50s - loss: 0.0970 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "130/250 [==============>...............] - ETA: 49s - loss: 0.0964 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "131/250 [==============>...............] - ETA: 49s - loss: 0.0976 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "132/250 [==============>...............] - ETA: 48s - loss: 0.0971 - accuracy: 0.9728\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "133/250 [==============>...............] - ETA: 48s - loss: 0.0971 - accuracy: 0.9727\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "134/250 [===============>..............] - ETA: 47s - loss: 0.0969 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "135/250 [===============>..............] - ETA: 47s - loss: 0.0973 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "136/250 [===============>..............] - ETA: 47s - loss: 0.0980 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "137/250 [===============>..............] - ETA: 46s - loss: 0.0981 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "138/250 [===============>..............] - ETA: 46s - loss: 0.0974 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "139/250 [===============>..............] - ETA: 45s - loss: 0.0979 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "140/250 [===============>..............] - ETA: 45s - loss: 0.0973 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "141/250 [===============>..............] - ETA: 44s - loss: 0.0969 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "142/250 [================>.............] - ETA: 44s - loss: 0.0983 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "143/250 [================>.............] - ETA: 44s - loss: 0.0994 - accuracy: 0.9714\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "144/250 [================>.............] - ETA: 43s - loss: 0.0988 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "145/250 [================>.............] - ETA: 43s - loss: 0.0998 - accuracy: 0.9713\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "146/250 [================>.............] - ETA: 42s - loss: 0.0992 - accuracy: 0.9715\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "147/250 [================>.............] - ETA: 42s - loss: 0.0986 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "148/250 [================>.............] - ETA: 42s - loss: 0.0984 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "149/250 [================>.............] - ETA: 41s - loss: 0.0983 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "150/250 [=================>............] - ETA: 41s - loss: 0.0981 - accuracy: 0.9715\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "151/250 [=================>............] - ETA: 41s - loss: 0.0980 - accuracy: 0.9714\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "152/250 [=================>............] - ETA: 40s - loss: 0.0985 - accuracy: 0.9714\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "153/250 [=================>............] - ETA: 40s - loss: 0.0980 - accuracy: 0.9714\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "154/250 [=================>............] - ETA: 39s - loss: 0.0975 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "155/250 [=================>............] - ETA: 39s - loss: 0.0968 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "156/250 [=================>............] - ETA: 39s - loss: 0.0964 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "157/250 [=================>............] - ETA: 38s - loss: 0.0960 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "158/250 [=================>............] - ETA: 38s - loss: 0.0955 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "159/250 [==================>...........] - ETA: 37s - loss: 0.0958 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "160/250 [==================>...........] - ETA: 37s - loss: 0.0956 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "161/250 [==================>...........] - ETA: 36s - loss: 0.0952 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "162/250 [==================>...........] - ETA: 36s - loss: 0.0948 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "163/250 [==================>...........] - ETA: 35s - loss: 0.0943 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.0938 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "165/250 [==================>...........] - ETA: 35s - loss: 0.0938 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "166/250 [==================>...........] - ETA: 34s - loss: 0.0933 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "167/250 [===================>..........] - ETA: 34s - loss: 0.0929 - accuracy: 0.9728\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "168/250 [===================>..........] - ETA: 33s - loss: 0.0929 - accuracy: 0.9727\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "169/250 [===================>..........] - ETA: 33s - loss: 0.0930 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "170/250 [===================>..........] - ETA: 32s - loss: 0.0931 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "171/250 [===================>..........] - ETA: 32s - loss: 0.0927 - accuracy: 0.9727\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "172/250 [===================>..........] - ETA: 32s - loss: 0.0927 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "173/250 [===================>..........] - ETA: 31s - loss: 0.0931 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "174/250 [===================>..........] - ETA: 31s - loss: 0.0926 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "175/250 [====================>.........] - ETA: 30s - loss: 0.0925 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "176/250 [====================>.........] - ETA: 30s - loss: 0.0927 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "177/250 [====================>.........] - ETA: 29s - loss: 0.0922 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "178/250 [====================>.........] - ETA: 29s - loss: 0.0930 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "179/250 [====================>.........] - ETA: 29s - loss: 0.0934 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "180/250 [====================>.........] - ETA: 28s - loss: 0.0929 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "181/250 [====================>.........] - ETA: 28s - loss: 0.0939 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "182/250 [====================>.........] - ETA: 28s - loss: 0.0935 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "183/250 [====================>.........] - ETA: 27s - loss: 0.0931 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "184/250 [=====================>........] - ETA: 27s - loss: 0.0929 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "185/250 [=====================>........] - ETA: 26s - loss: 0.0926 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "186/250 [=====================>........] - ETA: 26s - loss: 0.0921 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "187/250 [=====================>........] - ETA: 26s - loss: 0.0918 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "188/250 [=====================>........] - ETA: 25s - loss: 0.0921 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "189/250 [=====================>........] - ETA: 25s - loss: 0.0920 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "190/250 [=====================>........] - ETA: 24s - loss: 0.0922 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "191/250 [=====================>........] - ETA: 24s - loss: 0.0929 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "192/250 [======================>.......] - ETA: 23s - loss: 0.0931 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.0926 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "194/250 [======================>.......] - ETA: 23s - loss: 0.0922 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "195/250 [======================>.......] - ETA: 22s - loss: 0.0918 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "196/250 [======================>.......] - ETA: 22s - loss: 0.0925 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "197/250 [======================>.......] - ETA: 21s - loss: 0.0925 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "198/250 [======================>.......] - ETA: 21s - loss: 0.0920 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "199/250 [======================>.......] - ETA: 21s - loss: 0.0920 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "200/250 [=======================>......] - ETA: 20s - loss: 0.0923 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "201/250 [=======================>......] - ETA: 20s - loss: 0.0919 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "202/250 [=======================>......] - ETA: 19s - loss: 0.0918 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "203/250 [=======================>......] - ETA: 19s - loss: 0.0917 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "204/250 [=======================>......] - ETA: 18s - loss: 0.0916 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "205/250 [=======================>......] - ETA: 18s - loss: 0.0916 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "206/250 [=======================>......] - ETA: 18s - loss: 0.0917 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "207/250 [=======================>......] - ETA: 17s - loss: 0.0916 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "208/250 [=======================>......] - ETA: 17s - loss: 0.0912 - accuracy: 0.9720\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "209/250 [========================>.....] - ETA: 16s - loss: 0.0917 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "210/250 [========================>.....] - ETA: 16s - loss: 0.0926 - accuracy: 0.9713\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "211/250 [========================>.....] - ETA: 16s - loss: 0.0923 - accuracy: 0.9713\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "212/250 [========================>.....] - ETA: 15s - loss: 0.0919 - accuracy: 0.9715\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "213/250 [========================>.....] - ETA: 15s - loss: 0.0915 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "214/250 [========================>.....] - ETA: 14s - loss: 0.0916 - accuracy: 0.9716\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "215/250 [========================>.....] - ETA: 14s - loss: 0.0912 - accuracy: 0.9717\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "216/250 [========================>.....] - ETA: 14s - loss: 0.0910 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "217/250 [=========================>....] - ETA: 13s - loss: 0.0908 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "218/250 [=========================>....] - ETA: 13s - loss: 0.0910 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "219/250 [=========================>....] - ETA: 12s - loss: 0.0910 - accuracy: 0.9718\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "220/250 [=========================>....] - ETA: 12s - loss: 0.0906 - accuracy: 0.9719\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "221/250 [=========================>....] - ETA: 12s - loss: 0.0903 - accuracy: 0.9721\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "222/250 [=========================>....] - ETA: 11s - loss: 0.0900 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "223/250 [=========================>....] - ETA: 11s - loss: 0.0896 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "224/250 [=========================>....] - ETA: 10s - loss: 0.0894 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "225/250 [==========================>...] - ETA: 10s - loss: 0.0892 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "226/250 [==========================>...] - ETA: 9s - loss: 0.0888 - accuracy: 0.9724 \n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "227/250 [==========================>...] - ETA: 9s - loss: 0.0885 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "228/250 [==========================>...] - ETA: 9s - loss: 0.0890 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "229/250 [==========================>...] - ETA: 8s - loss: 0.0891 - accuracy: 0.9722\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "230/250 [==========================>...] - ETA: 8s - loss: 0.0888 - accuracy: 0.9723\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "231/250 [==========================>...] - ETA: 7s - loss: 0.0886 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "232/250 [==========================>...] - ETA: 7s - loss: 0.0883 - accuracy: 0.9724\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "233/250 [==========================>...] - ETA: 7s - loss: 0.0880 - accuracy: 0.9726\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "234/250 [===========================>..] - ETA: 6s - loss: 0.0878 - accuracy: 0.9725\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "235/250 [===========================>..] - ETA: 6s - loss: 0.0875 - accuracy: 0.9727\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "236/250 [===========================>..] - ETA: 5s - loss: 0.0874 - accuracy: 0.9727\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "237/250 [===========================>..] - ETA: 5s - loss: 0.0871 - accuracy: 0.9728\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "238/250 [===========================>..] - ETA: 4s - loss: 0.0867 - accuracy: 0.9729\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "239/250 [===========================>..] - ETA: 4s - loss: 0.0864 - accuracy: 0.9730\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "240/250 [===========================>..] - ETA: 4s - loss: 0.0861 - accuracy: 0.9731\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "241/250 [===========================>..] - ETA: 3s - loss: 0.0861 - accuracy: 0.9731\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "242/250 [============================>.] - ETA: 3s - loss: 0.0858 - accuracy: 0.9732\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "243/250 [============================>.] - ETA: 2s - loss: 0.0855 - accuracy: 0.9733\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "244/250 [============================>.] - ETA: 2s - loss: 0.0856 - accuracy: 0.9732\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "245/250 [============================>.] - ETA: 2s - loss: 0.0853 - accuracy: 0.9733\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "246/250 [============================>.] - ETA: 1s - loss: 0.0850 - accuracy: 0.9734\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "247/250 [============================>.] - ETA: 1s - loss: 0.0849 - accuracy: 0.9734\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "248/250 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9735\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9735\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "250/250 [==============================] - 145s 579ms/step - loss: 0.0858 - accuracy: 0.9733 - val_loss: 0.0461 - val_accuracy: 0.9889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    trainGen, steps_per_epoch=trainGen.samples // 32,\n",
    "\tvalidation_data=testGen, validation_steps=testGen.samples // 32,\n",
    "\tepochs=15, callbacks=[checkpoint, early])\n",
    "\n",
    "model.save(os.path.sep.join([config.OUTPUT_PATH, \"efficient.model\"]), save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "id": "OAiumzCB9HEz",
    "outputId": "c8199647-024f-4aea-eefb-3128417e24ca"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = np.arange(0, 15)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training and validation loss EfficientNet\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(os.path.sep.join([config.OUTPUT_PATH, \"efficientnet_losses.png\"]))\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, hist.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, hist.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training and validation accuracy EfficientNet\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(os.path.sep.join([config.OUTPUT_PATH, \"efficientnet_accuracy.png\"]))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I4yLSvmg9HEz",
    "outputId": "8228664b-277a-4752-8dc0-282a0db2aa43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        open      0.977     0.966     0.972       388\n",
      "       short      0.980     0.953     0.966       301\n",
      "    mousebit      0.979     0.936     0.957       393\n",
      "        spur      0.929     0.972     0.950       325\n",
      "      copper      0.961     1.000     0.980       294\n",
      "    pin-hole      0.980     0.987     0.983       300\n",
      "\n",
      "    accuracy                          0.968      2001\n",
      "   macro avg      0.968     0.969     0.968      2001\n",
      "weighted avg      0.968     0.968     0.967      2001\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = list()\n",
    "for path in testGen.filepaths:\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    preds = model.predict(image)\n",
    "    predictions.append(preds.argmax(axis=1))\n",
    "\n",
    "print(classification_report(testGen.classes,\n",
    "                            predictions, target_names=testGen.class_indices, digits=3))\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.grid(False)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "font = {'size' : 12}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cm = confusion_matrix(testGen.classes, predictions)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(testGen.class_indices))\n",
    "\n",
    "plt.xticks(tick_marks, testGen.class_indices, rotation=45)\n",
    "plt.yticks(tick_marks, testGen.class_indices)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted Defect')\n",
    "plt.ylabel('True Defect')\n",
    "plt.title('Confusion matrix EfficientNetB0') # Change title\n",
    "plt.savefig('output/efficientnet_confusion_matrix.png') # Change file name\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tD6Tq_c9HEz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOS3lVk89HEz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "interpreter": {
   "hash": "25c84cac0d85cf2b51a97d2bd7b0fa6585599b14f00604962e71b8ec1671851d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
